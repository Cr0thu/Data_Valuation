{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import sqrtm\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset, ConcatDataset\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import copy\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torchvision.models as models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "resnet18.fc = torch.nn.Identity()\n",
    "resnet18.to(device).eval()\n",
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# penalty = 4.282\n",
    "penalty = 100\n",
    "eps = 0.01\n",
    "# width = 0.15\n",
    "# gap = 0.05\n",
    "# MCsample = 500\n",
    "# sigma = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already allocated: 226.572265625 MB\n",
      "tensor(227.4364, device='cuda:0') torch.Size([4000])\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# normalize = transforms.Normalize(mean=[0.559, 0.571, 0.586],\n",
    "#                                  std=[0.230, 0.226, 0.249])\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "labels = np.load('../CIFAR-10-C/labels.npy')\n",
    "selected_indices_0 = labels == 0\n",
    "selected_indices_1 = labels == 1\n",
    "\n",
    "def data_preprocess(images):\n",
    "    images_0 = torch.stack([preprocess(image) for image in images[selected_indices_0]]).to(device)\n",
    "    images_1 = torch.stack([preprocess(image) for image in images[selected_indices_1]]).to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding_0 = resnet18(images_0)\n",
    "        embedding_1 = resnet18(images_1)\n",
    "        embedding_0 = torch.concatenate([embedding_0, torch.ones(embedding_0.size()[0],1).to(device)], dim=1)\n",
    "        embedding_1 = torch.concatenate([embedding_1, torch.ones(embedding_1.size()[0],1).to(device)], dim=1)\n",
    "        perm = torch.randperm(len(embedding_0))\n",
    "        embedding_0 = embedding_0[perm]\n",
    "        embedding_1 = embedding_1[perm]\n",
    "        # print(embedding_0.size(), embedding_1.size())\n",
    "    return embedding_0, embedding_1\n",
    "\n",
    "\n",
    "images = np.load('../CIFAR-10-C/brightness.npy')\n",
    "images_a_0_embedding, images_a_1_embedding = data_preprocess(images)\n",
    "images = np.load('../CIFAR-10-C/contrast.npy')\n",
    "images_b_0_embedding, images_b_1_embedding = data_preprocess(images)\n",
    "# images = np.load('../CIFAR-10-C/defocus_blur.npy')\n",
    "# images_c_0_embedding, images_c_1_embedding = data_preprocess(images)\n",
    "# images = np.load('../CIFAR-10-C/elastic_transform.npy')\n",
    "# images_d_0_embedding, images_d_1_embedding = data_preprocess(images)\n",
    "# images = np.load('../CIFAR-10-C/fog.npy')\n",
    "# images_e_0_embedding, images_e_1_embedding = data_preprocess(images)\n",
    "# images = np.load('../CIFAR-10-C/frost.npy')\n",
    "# images_f_0_embedding, images_f_1_embedding = data_preprocess(images)\n",
    "# images = np.load('../CIFAR-10-C/gaussian_blur.npy')\n",
    "# images_g_0_embedding, images_g_1_embedding = data_preprocess(images)\n",
    "# images = np.load('../CIFAR-10-C/gaussian_noise.npy')\n",
    "# images_h_0_embedding, images_h_1_embedding = data_preprocess(images)\n",
    "# images = np.load('../CIFAR-10-C/glass_blur.npy')\n",
    "# images_i_0_embedding, images_i_1_embedding = data_preprocess(images)\n",
    "# images = np.load('../CIFAR-10-C/impulse_noise.npy')\n",
    "# images_j_0_embedding, images_j_1_embedding = data_preprocess(images)\n",
    "# Assuming the embeddings are loaded as numpy arrays\n",
    "datasets = [\n",
    "    images_a_0_embedding, \n",
    "    images_a_1_embedding, \n",
    "    images_b_0_embedding, \n",
    "    images_b_1_embedding\n",
    "]\n",
    "\n",
    "# Concatenate all datasets to calculate the mean and standard deviation\n",
    "combined_data = torch.cat(datasets, dim=0)  # Shape: (20000, 513)\n",
    "\n",
    "# Compute mean and std across the feature dimension (dim=0)\n",
    "mean = combined_data.mean(dim=0, keepdim=True)\n",
    "std = combined_data.std(dim=0, keepdim=True)\n",
    "\n",
    "# Normalize each dataset using the same mean and std\n",
    "normalized_datasets = [(dataset - mean) / (std + 1e-6) for dataset in datasets]\n",
    "\n",
    "# Unpack the normalized datasets\n",
    "images_a_0_embedding, images_a_1_embedding, images_b_0_embedding, images_b_1_embedding = normalized_datasets\n",
    "\n",
    "P_x = torch.concatenate([\n",
    "    images_a_0_embedding[4000:],\n",
    "    images_b_0_embedding[4000:],\n",
    "    # images_c_0_embedding[4000:],\n",
    "    # images_d_0_embedding[4000:],\n",
    "    # images_e_0_embedding[4000:],\n",
    "    # images_f_0_embedding[4000:],\n",
    "    # images_g_0_embedding[4000:],\n",
    "    # images_h_0_embedding[4000:],\n",
    "    # images_i_0_embedding[4000:],\n",
    "    # images_j_0_embedding[4000:],\n",
    "    images_a_1_embedding[4000:],\n",
    "    images_b_1_embedding[4000:],\n",
    "    # images_c_1_embedding[4000:],\n",
    "    # images_d_1_embedding[4000:],\n",
    "    # images_e_1_embedding[4000:],\n",
    "    # images_f_1_embedding[4000:],\n",
    "    # images_g_1_embedding[4000:],\n",
    "    # images_h_1_embedding[4000:],\n",
    "    # images_i_1_embedding[4000:],\n",
    "    # images_j_1_embedding[4000:]\n",
    "])\n",
    "# P_y = torch.concatenate([torch.zeros(10000), torch.ones(10000)]).to(device)\n",
    "P_y = torch.concatenate([torch.zeros(2000), torch.ones(2000)]).to(device)\n",
    "Q_x = torch.concatenate([\n",
    "    images_a_0_embedding[:4000],\n",
    "    images_b_0_embedding[:4000],\n",
    "    # images_c_0_embedding[:4000],\n",
    "    # images_d_0_embedding[:4000],\n",
    "    # images_e_0_embedding[:4000],\n",
    "    # images_f_0_embedding[:4000],\n",
    "    # images_g_0_embedding[:4000],\n",
    "    # images_h_0_embedding[:4000],\n",
    "    # images_i_0_embedding[:4000],\n",
    "    # images_j_0_embedding[:4000],\n",
    "    images_a_1_embedding[:4000],\n",
    "    images_b_1_embedding[:4000],\n",
    "    # images_c_1_embedding[:4000],\n",
    "    # images_d_1_embedding[:4000],\n",
    "    # images_e_1_embedding[:4000],\n",
    "    # images_f_1_embedding[:4000],\n",
    "    # images_g_1_embedding[:4000],\n",
    "    # images_h_1_embedding[:4000],\n",
    "    # images_i_1_embedding[:4000],\n",
    "    # images_j_1_embedding[:4000]\n",
    "])\n",
    "# Q_y = torch.concatenate([torch.zeros(40000), torch.ones(40000)]).to(device)\n",
    "Q_y = torch.concatenate([torch.zeros(8000), torch.ones(8000)]).to(device)\n",
    "# del resnet18\n",
    "allocated_memory = torch.cuda.memory_allocated()\n",
    "print(f\"Already allocated: {allocated_memory / 1024 ** 2} MB\")\n",
    "print(P_x[0].sum(), P_y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset:\n",
    "    def __init__(self, X, y, num, score, base_loss, base_acc, noise, bias, bias_loss, bias_acc):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.num = num\n",
    "        self.score = score\n",
    "        self.base_loss = base_loss\n",
    "        self.base_acc = base_acc\n",
    "        self.noise = noise\n",
    "        self.bias = bias\n",
    "        self.bias_loss = bias_loss\n",
    "        self.bias_acc = bias_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + torch.exp(-z))\n",
    "\n",
    "def generate_data_cifar10(indices, dataset, label, noise):\n",
    "    X = dataset[indices]\n",
    "    y = torch.ones(X.size()[0]) * label\n",
    "    y = torch.tensor(y, dtype=torch.int)\n",
    "    y = y.to(device)\n",
    "    y[:int(noise*X.size()[0])] = 1 - y[:int(noise*X.size()[0])]\n",
    "    # print(y)\n",
    "    return X, y, X.size()[0]\n",
    "\n",
    "# def generate_dataset_cifar10(sample_indices, noise):\n",
    "    # perm_indices = torch.randperm(80000).to(device)\n",
    "    # sample_indices = perm_indices[:N]\n",
    "    a_0 = sample_indices[(sample_indices >= 0) & (sample_indices < 4000)]\n",
    "    b_0 = sample_indices[(sample_indices >= 4000) & (sample_indices < 8000)] - 4000\n",
    "    c_0 = sample_indices[(sample_indices >= 8000) & (sample_indices < 12000)] - 8000\n",
    "    d_0 = sample_indices[(sample_indices >= 12000) & (sample_indices < 16000)] - 12000\n",
    "    e_0 = sample_indices[(sample_indices >= 16000) & (sample_indices < 20000)] - 16000\n",
    "    f_0 = sample_indices[(sample_indices >= 20000) & (sample_indices < 24000)] - 20000\n",
    "    g_0 = sample_indices[(sample_indices >= 24000) & (sample_indices < 28000)] - 24000\n",
    "    h_0 = sample_indices[(sample_indices >= 28000) & (sample_indices < 32000)] - 28000\n",
    "    i_0 = sample_indices[(sample_indices >= 32000) & (sample_indices < 36000)] - 32000\n",
    "    j_0 = sample_indices[(sample_indices >= 36000) & (sample_indices < 40000)] - 36000\n",
    "    a_1 = sample_indices[(sample_indices >= 40000) & (sample_indices < 44000)] - 40000\n",
    "    b_1 = sample_indices[(sample_indices >= 44000) & (sample_indices < 48000)] - 44000\n",
    "    c_1 = sample_indices[(sample_indices >= 48000) & (sample_indices < 52000)] - 48000\n",
    "    d_1 = sample_indices[(sample_indices >= 52000) & (sample_indices < 56000)] - 52000\n",
    "    e_1 = sample_indices[(sample_indices >= 56000) & (sample_indices < 60000)] - 56000\n",
    "    f_1 = sample_indices[(sample_indices >= 60000) & (sample_indices < 64000)] - 60000\n",
    "    g_1 = sample_indices[(sample_indices >= 64000) & (sample_indices < 68000)] - 64000\n",
    "    h_1 = sample_indices[(sample_indices >= 68000) & (sample_indices < 72000)] - 68000\n",
    "    i_1 = sample_indices[(sample_indices >= 72000) & (sample_indices < 76000)] - 72000\n",
    "    j_1 = sample_indices[(sample_indices >= 76000) & (sample_indices < 80000)] - 76000\n",
    "    a_X_0, a_y_0, a_num_0 = generate_data_cifar10(a_0, images_a_0_embedding[:4000], 0, noise)\n",
    "    a_X_1, a_y_1, a_num_1 = generate_data_cifar10(a_1, images_a_1_embedding[:4000], 1, noise)\n",
    "    b_X_0, b_y_0, b_num_0 = generate_data_cifar10(b_0, images_b_0_embedding[:4000], 0, noise)\n",
    "    b_X_1, b_y_1, b_num_1 = generate_data_cifar10(b_1, images_b_1_embedding[:4000], 1, noise)\n",
    "    c_X_0, c_y_0, c_num_0 = generate_data_cifar10(c_0, images_c_0_embedding[:4000], 0, noise)\n",
    "    c_X_1, c_y_1, c_num_1 = generate_data_cifar10(c_1, images_c_1_embedding[:4000], 1, noise)\n",
    "    d_X_0, d_y_0, d_num_0 = generate_data_cifar10(d_0, images_d_0_embedding[:4000], 0, noise)\n",
    "    d_X_1, d_y_1, d_num_1 = generate_data_cifar10(d_1, images_d_1_embedding[:4000], 1, noise)\n",
    "    e_X_0, e_y_0, e_num_0 = generate_data_cifar10(e_0, images_e_0_embedding[:4000], 0, noise)\n",
    "    e_X_1, e_y_1, e_num_1 = generate_data_cifar10(e_1, images_e_1_embedding[:4000], 1, noise)\n",
    "    f_X_0, f_y_0, f_num_0 = generate_data_cifar10(f_0, images_f_0_embedding[:4000], 0, noise)\n",
    "    f_X_1, f_y_1, f_num_1 = generate_data_cifar10(f_1, images_f_1_embedding[:4000], 1, noise)\n",
    "    g_X_0, g_y_0, g_num_0 = generate_data_cifar10(g_0, images_g_0_embedding[:4000], 0, noise)\n",
    "    g_X_1, g_y_1, g_num_1 = generate_data_cifar10(g_1, images_g_1_embedding[:4000], 1, noise)\n",
    "    h_X_0, h_y_0, h_num_0 = generate_data_cifar10(h_0, images_h_0_embedding[:4000], 0, noise)\n",
    "    h_X_1, h_y_1, h_num_1 = generate_data_cifar10(h_1, images_h_1_embedding[:4000], 1, noise)\n",
    "    i_X_0, i_y_0, i_num_0 = generate_data_cifar10(i_0, images_i_0_embedding[:4000], 0, noise)\n",
    "    i_X_1, i_y_1, i_num_1 = generate_data_cifar10(i_1, images_i_1_embedding[:4000], 1, noise)\n",
    "    j_X_0, j_y_0, j_num_0 = generate_data_cifar10(j_0, images_j_0_embedding[:4000], 0, noise)\n",
    "    j_X_1, j_y_1, j_num_1 = generate_data_cifar10(j_1, images_j_1_embedding[:4000], 1, noise)\n",
    "    X = torch.concatenate([\n",
    "        a_X_0,\n",
    "        b_X_0,\n",
    "        c_X_0,\n",
    "        d_X_0,\n",
    "        e_X_0,\n",
    "        f_X_0,\n",
    "        g_X_0,\n",
    "        h_X_0,\n",
    "        i_X_0,\n",
    "        j_X_0,\n",
    "        a_X_1,\n",
    "        b_X_1,\n",
    "        c_X_1,\n",
    "        d_X_1,\n",
    "        e_X_1,\n",
    "        f_X_1,\n",
    "        g_X_1,\n",
    "        h_X_1,\n",
    "        i_X_1,\n",
    "        j_X_1\n",
    "    ], axis = 0)\n",
    "    y = torch.concatenate([\n",
    "        a_y_0,\n",
    "        b_y_0,\n",
    "        c_y_0,\n",
    "        d_y_0,\n",
    "        e_y_0,\n",
    "        f_y_0,\n",
    "        g_y_0,\n",
    "        h_y_0,\n",
    "        i_y_0,\n",
    "        j_y_0,\n",
    "        a_y_1,\n",
    "        b_y_1,\n",
    "        c_y_1,\n",
    "        d_y_1,\n",
    "        e_y_1,\n",
    "        f_y_1,\n",
    "        g_y_1,\n",
    "        h_y_1,\n",
    "        i_y_1,\n",
    "        j_y_1\n",
    "    ], axis = 0)\n",
    "    bias = torch.tensor([\n",
    "        a_num_0+a_num_1,\n",
    "        b_num_0+b_num_1,\n",
    "        c_num_0+c_num_1,\n",
    "        d_num_0+d_num_1,\n",
    "        e_num_0+e_num_1,\n",
    "        f_num_0+f_num_1,\n",
    "        g_num_0+g_num_1,\n",
    "        h_num_0+h_num_1,\n",
    "        i_num_0+i_num_1,\n",
    "        j_num_0+j_num_1\n",
    "    ]).to(device)\n",
    "    return bias, X, y\n",
    "\n",
    "def generate_train_cifar10(train_size_a_0, train_size_a_1, train_size_b_0, train_size_b_1, train_number, noise_level):\n",
    "    train_dataset = []\n",
    "    bias = torch.tensor([\n",
    "        train_size_a_0, train_size_b_0,\n",
    "        train_size_a_1, train_size_b_1\n",
    "    ]).to(device)\n",
    "    for i in range(train_number):\n",
    "        perm_indices_a_0 = torch.randperm(4000).to(device)\n",
    "        sample_indices_a_0 = perm_indices_a_0[:train_size_a_0]\n",
    "        perm_indices_a_1 = torch.randperm(4000).to(device)\n",
    "        sample_indices_a_1 = perm_indices_a_1[:train_size_a_1]\n",
    "        perm_indices_b_0 = torch.randperm(4000).to(device)\n",
    "        sample_indices_b_0 = perm_indices_b_0[:train_size_b_0]\n",
    "        perm_indices_b_1 = torch.randperm(4000).to(device)\n",
    "        sample_indices_b_1 = perm_indices_b_1[:train_size_b_1]\n",
    "        train_X_a_0, train_y_a_0, _ = generate_data_cifar10(sample_indices_a_0, images_a_0_embedding[:4000], 0, eps * noise_level)\n",
    "        train_X_a_1, train_y_a_1, _ = generate_data_cifar10(sample_indices_a_1, images_a_1_embedding[:4000], 1, eps * noise_level)\n",
    "        train_X_b_0, train_y_b_0, _ = generate_data_cifar10(sample_indices_b_0, images_b_0_embedding[:4000], 0, eps * noise_level)\n",
    "        train_X_b_1, train_y_b_1, _ = generate_data_cifar10(sample_indices_b_1, images_b_1_embedding[:4000], 1, eps * noise_level)\n",
    "        train_X = torch.concatenate([train_X_a_0, train_X_b_0, train_X_a_1, train_X_b_1])\n",
    "        train_y = torch.concatenate([train_y_a_0, train_y_b_0, train_y_a_1, train_y_b_1])\n",
    "        train_dataset.append(dataset(train_X, train_y, train_X.size()[0], 0, 0, 0, noise_level*eps, bias, [], []))\n",
    "    return train_dataset\n",
    "\n",
    "def subsample(X, y, size):\n",
    "    perm = torch.randperm(len(y))\n",
    "    sample_X = X[perm[:size]]\n",
    "    sample_y = y[perm[:size]]\n",
    "    return sample_X, sample_y\n",
    "\n",
    "def compute_hessian(mu, X):\n",
    "    sigm = sigmoid(X @ mu.t())\n",
    "    diag_sigm = (sigm * (1 - sigm)).flatten()\n",
    "    res = torch.eye(X.size(1), device=device)/penalty\n",
    "    res += (X.t() * diag_sigm) @ X\n",
    "    return res\n",
    "\n",
    "def compute_score(mu0, Q0, lg0, mu1, Q1, lg1, mu2, Q2, lg2):\n",
    "    Q = Q1 + Q2 - Q0\n",
    "    Q_t_L = torch.linalg.cholesky(Q)\n",
    "    Q_t_L_inv = torch.linalg.solve_triangular(Q_t_L, torch.eye(Q_t_L.size(0), device=device), upper=False)\n",
    "    Q_inv = Q_t_L_inv.T @ Q_t_L_inv\n",
    "    mu = torch.matmul(Q_inv, torch.matmul(Q1, mu1) + torch.matmul(Q2, mu2) - torch.matmul(Q0, mu0))\n",
    "\n",
    "    lg12 = 2 * torch.sum(torch.log(torch.diagonal(Q_t_L)))\n",
    "\n",
    "    lg = lg1+lg2-lg12-lg0\n",
    "\n",
    "    sqr = torch.matmul(mu.T, torch.matmul(Q, mu)) - torch.matmul(mu1.T, torch.matmul(Q1, mu1)) - torch.matmul(mu2.T, torch.matmul(Q2, mu2)) + torch.matmul(mu0.T, torch.matmul(Q0, mu0))\n",
    "\n",
    "    score = 0.5 * (lg + sqr)\n",
    "    # print(lg1,lg2,lg12,lg0,sqr)\n",
    "    return score.item()\n",
    "\n",
    "def compute_data_score_err(mu_test, Q_test, test_X, test_y, train_X, train_y, lg2, bias):\n",
    "    test_N = test_y.size()[0]\n",
    "    M = test_X.size()[1]\n",
    "    test_size_a_0, test_size_b_0, test_size_a_1, test_size_b_1 = bias\n",
    "\n",
    "    mu0 = torch.zeros((1, M))\n",
    "    mu0 = mu0.to(device)\n",
    "    Q0 = torch.eye(M)/penalty\n",
    "    Q0 = Q0.to(device)\n",
    "    lg0 = -M * torch.log(torch.tensor(penalty))\n",
    "    \n",
    "    train = LogisticRegression(fit_intercept = False, C = penalty, max_iter=5000).fit(train_X.cpu(), train_y.cpu())\n",
    "    # print(train.score(torch.cat([Q_x[1000:2000], Q_x[5000:6000], Q_x[9000:10000], Q_x[13000:14000]]).cpu(), torch.cat([Q_y[1000:2000], Q_y[5000:6000], Q_y[9000:10000], Q_y[13000:14000]]).cpu()))\n",
    "    # print(train.score(P_x.cpu(), P_y.cpu()))\n",
    "    mu_train = torch.tensor(train.coef_, dtype=torch.float32, device=device)\n",
    "    # mu_train_numpy = mu_train.detach().squeeze().cpu().numpy()\n",
    "\n",
    "    Q_train = compute_hessian(mu_train, train_X)\n",
    "    Q_train_L = torch.linalg.cholesky(Q_train)\n",
    "    # Q_train_inverse = Q_train_L_inv.T @ Q_train_L_inv\n",
    "    # Q_train_inverse = torch.inverse(Q_train)\n",
    "    # Q_numpy = Q_train_inverse.detach().cpu().numpy()\n",
    "\n",
    "    lg1 = 2 * torch.sum(torch.log(torch.diagonal(Q_train_L)))\n",
    "\n",
    "    score = compute_score(mu0.t(), Q0, lg0, mu_train.t(), Q_train, lg1, mu_test.t(), Q_test, lg2)\n",
    "\n",
    "    test_y = test_y.float()\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    base_predictive = sigmoid(torch.matmul(test_X, mu_train.t())).squeeze()\n",
    "    base_predictions = (base_predictive >= 0.5).float()\n",
    "    base_loss = criterion(base_predictive, test_y)\n",
    "    base_acc = (base_predictions == test_y).float().mean()\n",
    "\n",
    "    base_predictive_0 = sigmoid(torch.matmul(test_X[:test_size_a_0], mu_train.t())).squeeze()\n",
    "    base_predictions_0 = (base_predictive_0 >= 0.5).float()\n",
    "    base_loss_0 = criterion(base_predictive_0, test_y[:test_size_a_0])\n",
    "    base_acc_0 = (base_predictions_0 == test_y[:test_size_a_0]).float().mean()\n",
    "\n",
    "    base_predictive_1 = sigmoid(torch.matmul(test_X[test_size_a_0:test_size_a_0+test_size_b_0], mu_train.t())).squeeze()\n",
    "    base_predictions_1 = (base_predictive_1 >= 0.5).float()\n",
    "    base_loss_1 = criterion(base_predictive_1, test_y[test_size_a_0:test_size_a_0+test_size_b_0])\n",
    "    base_acc_1 = (base_predictions_1 == test_y[test_size_a_0:test_size_a_0+test_size_b_0]).float().mean()\n",
    "\n",
    "    base_predictive_2 = sigmoid(torch.matmul(test_X[test_size_a_0+test_size_b_0:test_size_a_0+test_size_b_0+test_size_a_1], mu_train.t())).squeeze()\n",
    "    base_predictions_2 = (base_predictive_2 >= 0.5).float()\n",
    "    base_loss_2 = criterion(base_predictive_2, test_y[test_size_a_0+test_size_b_0:test_size_a_0+test_size_b_0+test_size_a_1])\n",
    "    base_acc_2 = (base_predictions_2 == test_y[test_size_a_0+test_size_b_0:test_size_a_0+test_size_b_0+test_size_a_1]).float().mean()\n",
    "\n",
    "    base_predictive_3 = sigmoid(torch.matmul(test_X[test_size_a_0+test_size_b_0+test_size_a_1:], mu_train.t())).squeeze()\n",
    "    base_predictions_3 = (base_predictive_3 >= 0.5).float()\n",
    "    base_loss_3 = criterion(base_predictive_3, test_y[test_size_a_0+test_size_b_0+test_size_a_1:])\n",
    "    base_acc_3 = (base_predictions_3 == test_y[test_size_a_0+test_size_b_0+test_size_a_1:]).float().mean()\n",
    "    \n",
    "    return score, base_loss.item(), base_acc.item(), np.array([base_loss_0.item(), base_loss_1.item(), base_loss_2.item(), base_loss_3.item()]), np.array([base_acc_0.item(), base_acc_1.item(), base_acc_2.item(), base_acc_3.item()])\n",
    "\n",
    "def get_err_score(train_data, test_X, test_y, train_number, test_bias):\n",
    "    test = LogisticRegression(fit_intercept = False, C = penalty, max_iter=5000).fit(test_X.cpu(), test_y.cpu())\n",
    "    mu_test = torch.tensor(test.coef_, dtype=torch.float32, device=device)\n",
    "    Q_test = compute_hessian(mu_test, test_X)\n",
    "\n",
    "    L = torch.linalg.cholesky(Q_test)\n",
    "    lg2 = 2 * torch.sum(torch.log(torch.diagonal(L)))\n",
    "\n",
    "    for i in range(train_number):\n",
    "        train_data[i].score, train_data[i].base_loss, train_data[i].base_acc, train_data[i].bias_loss, train_data[i].bias_acc = compute_data_score_err(mu_test, Q_test, test_X, test_y, train_data[i].X, train_data[i].y, lg2, test_bias)\n",
    "\n",
    "\n",
    "# def random_copy(train_data, copy_num, num_candidate, train_size):\n",
    "#     new_train_data = []\n",
    "#     for i in range(num_candidate):\n",
    "#         perm_indices = torch.randperm(train_size)\n",
    "#         sample_indices = perm_indices[:copy_num]\n",
    "#         new_train_X = torch.concatenate([train_data[i].X, train_data[i].X[sample_indices]])\n",
    "#         new_train_y = torch.concatenate([train_data[i].y, train_data[i].y[sample_indices]])\n",
    "#         new_train_data.append(dataset(new_train_X, new_train_y, new_train_X.size()[0], 0, 0, 0, 0, 0, 0, train_data[i].noise_ratio, train_data[i].label_ratio, train_data[i].bias_ratio))\n",
    "#     return new_train_data\n",
    "\n",
    "def mimic_label_copy(train_data, num_candidate, test_ratio):\n",
    "    new_train_data = []\n",
    "    test_a_label_ratio = test_ratio[0]/test_ratio[2]\n",
    "    test_b_label_ratio = test_ratio[1]/test_ratio[3]\n",
    "    for i in range(num_candidate):\n",
    "        train_size_a_0, train_size_b_0, train_size_a_1, train_size_b_1 = train_data[i].bias\n",
    "        target_size_a_0 = int(train_size_a_1 * test_a_label_ratio)\n",
    "        target_size_b_0 = int(train_size_b_1 * test_b_label_ratio)\n",
    "\n",
    "        if train_size_a_0 < target_size_a_0:\n",
    "            num_extra = int(target_size_a_0/train_size_a_0) - 1\n",
    "            # extra_indices = torch.randint(0, train_size_a_0, (num_extra,))\n",
    "            indices = torch.range(0,train_size_a_0-1)\n",
    "            extra_indices = torch.tensor([])\n",
    "            for g in range(num_extra):\n",
    "                extra_indices = torch.cat([extra_indices, indices])\n",
    "            extra_indices = extra_indices.to(torch.long)\n",
    "            new_train_X_a_0 = torch.cat([train_data[i].X[:train_size_a_0], train_data[i].X[:train_size_a_0][extra_indices]])\n",
    "            new_train_X_a_1 = train_data[i].X[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1]\n",
    "            new_train_y_a_0 = torch.cat([train_data[i].y[:train_size_a_0], train_data[i].y[:train_size_a_0][extra_indices]])\n",
    "            new_train_y_a_1 = train_data[i].y[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1]\n",
    "        else:\n",
    "            target_size_a_1 = int(train_size_a_0 / test_a_label_ratio)\n",
    "            num_extra = int(target_size_a_1/train_size_a_1) - 1\n",
    "            # extra_indices = torch.randint(0, train_size_a_1, (num_extra,))\n",
    "            indices = torch.range(0,train_size_a_1-1)\n",
    "            extra_indices = torch.tensor([])\n",
    "            for g in range(num_extra):\n",
    "                extra_indices = torch.cat([extra_indices, indices])\n",
    "            extra_indices = extra_indices.to(torch.long)\n",
    "            new_train_X_a_1 = torch.cat([train_data[i].X[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1], train_data[i].X[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1][extra_indices]])\n",
    "            new_train_X_a_0 = train_data[i].X[:train_size_a_0]\n",
    "            new_train_y_a_1 = torch.cat([train_data[i].y[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1], train_data[i].y[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1][extra_indices]])\n",
    "            new_train_y_a_0 = train_data[i].y[:train_size_a_0]\n",
    "\n",
    "        if train_size_b_0 < target_size_b_0:\n",
    "            num_extra = int(target_size_b_0/train_size_b_0) - 1\n",
    "            # extra_indices = torch.randint(0, train_size_b_0, (num_extra,))\n",
    "            indices = torch.range(0,train_size_b_0-1)\n",
    "            extra_indices = torch.tensor([])\n",
    "            for g in range(num_extra):\n",
    "                extra_indices = torch.cat([extra_indices, indices])\n",
    "            extra_indices = extra_indices.to(torch.long)\n",
    "            new_train_X_b_0 = torch.cat([train_data[i].X[train_size_a_0:train_size_a_0+train_size_b_0], train_data[i].X[train_size_a_0:train_size_a_0+train_size_b_0][extra_indices]])\n",
    "            new_train_X_b_1 = train_data[i].X[train_size_a_0+train_size_b_0+train_size_a_1:]\n",
    "            new_train_y_b_0 = torch.cat([train_data[i].y[train_size_a_0:train_size_a_0+train_size_b_0], train_data[i].y[train_size_a_0:train_size_a_0+train_size_b_0][extra_indices]])\n",
    "            new_train_y_b_1 = train_data[i].y[train_size_a_0+train_size_b_0+train_size_a_1:]\n",
    "        else:\n",
    "            target_size_b_1 = int(train_size_b_0 / test_b_label_ratio)\n",
    "            num_extra = int(target_size_b_1/train_size_b_1) - 1\n",
    "            # extra_indices = torch.randint(0, train_size_b_1, (num_extra,))\n",
    "            indices = torch.range(0,train_size_b_1-1)\n",
    "            extra_indices = torch.tensor([])\n",
    "            for g in range(num_extra):\n",
    "                extra_indices = torch.cat([extra_indices, indices])\n",
    "            extra_indices = extra_indices.to(torch.long)\n",
    "            new_train_X_b_1 = torch.cat([train_data[i].X[train_size_a_0+train_size_b_0+train_size_a_1:], train_data[i].X[train_size_a_0+train_size_b_0+train_size_a_1:][extra_indices]])\n",
    "            new_train_X_b_0 = train_data[i].X[train_size_a_0:train_size_a_0+train_size_b_0]\n",
    "            new_train_y_b_1 = torch.cat([train_data[i].y[train_size_a_0+train_size_b_0+train_size_a_1:], train_data[i].y[train_size_a_0+train_size_b_0+train_size_a_1:][extra_indices]])\n",
    "            new_train_y_b_0 = train_data[i].y[train_size_a_0:train_size_a_0+train_size_b_0]\n",
    "\n",
    "        new_train_X = torch.concatenate([new_train_X_a_0, new_train_X_b_0, new_train_X_a_1, new_train_X_b_1])\n",
    "        new_train_y = torch.concatenate([new_train_y_a_0, new_train_y_b_0, new_train_y_a_1, new_train_y_b_1])\n",
    "        new_train_data.append(dataset(new_train_X, new_train_y, new_train_X.size()[0], 0, 0, 0, train_data[i].noise, torch.tensor([new_train_X_a_0.size()[0], new_train_X_b_0.size()[0], new_train_X_a_1.size()[0], new_train_X_b_1.size()[0]]).to(device), [], []))\n",
    "        # print(torch.tensor([new_train_X_a_0.size(), new_train_X_b_0.size(), new_train_X_a_1.size(), new_train_X_b_1.size()]).to(device))\n",
    "    return new_train_data\n",
    "\n",
    "def mimic_label_delete(train_data, num_candidate, test_ratio):\n",
    "    new_train_data = []\n",
    "    test_a_label_ratio = test_ratio[0]/test_ratio[2]\n",
    "    test_b_label_ratio = test_ratio[1]/test_ratio[3]\n",
    "    for i in range(num_candidate):\n",
    "        train_size_a_0, train_size_b_0, train_size_a_1, train_size_b_1 = train_data[i].bias\n",
    "        target_size_a_0 = int(train_size_a_1 * test_a_label_ratio)\n",
    "        target_size_b_0 = int(train_size_b_1 * test_b_label_ratio)\n",
    "\n",
    "        if train_size_a_0 > target_size_a_0:\n",
    "            residual_indices = torch.randperm(train_size_a_0)[:target_size_a_0]\n",
    "            new_train_X_a_0 = train_data[i].X[:train_size_a_0][residual_indices]\n",
    "            new_train_X_a_1 = train_data[i].X[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1]\n",
    "            new_train_y_a_0 = train_data[i].y[:train_size_a_0][residual_indices]\n",
    "            new_train_y_a_1 = train_data[i].y[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1]\n",
    "        else:\n",
    "            target_size_a_1 = int(train_size_a_0 / test_a_label_ratio)\n",
    "            residual_indices = torch.randperm(train_size_a_1)[:target_size_a_1]\n",
    "            new_train_X_a_1 = train_data[i].X[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1][residual_indices]\n",
    "            new_train_X_a_0 = train_data[i].X[:train_size_a_0]\n",
    "            new_train_y_a_1 = train_data[i].y[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1][residual_indices]\n",
    "            new_train_y_a_0 = train_data[i].y[:train_size_a_0]\n",
    "\n",
    "        if train_size_b_0 > target_size_b_0:\n",
    "            residual_indices = torch.randperm(train_size_b_0)[:target_size_b_0]\n",
    "            new_train_X_b_0 = train_data[i].X[train_size_a_0:train_size_a_0+train_size_b_0][residual_indices]\n",
    "            new_train_X_b_1 = train_data[i].X[train_size_a_0+train_size_b_0+train_size_a_1:]\n",
    "            new_train_y_b_0 = train_data[i].y[train_size_a_0:train_size_a_0+train_size_b_0][residual_indices]\n",
    "            new_train_y_b_1 = train_data[i].y[train_size_a_0+train_size_b_0+train_size_a_1:]\n",
    "        else:\n",
    "            target_size_b_1 = int(train_size_b_0 / test_b_label_ratio)\n",
    "            residual_indices = torch.randperm(train_size_b_1)[:target_size_b_1]\n",
    "            new_train_X_b_1 = train_data[i].X[train_size_a_0+train_size_b_0+train_size_a_1:][residual_indices]\n",
    "            new_train_X_b_0 = train_data[i].X[train_size_a_0:train_size_a_0+train_size_b_0]\n",
    "            new_train_y_b_1 = train_data[i].y[train_size_a_0+train_size_b_0+train_size_a_1:][residual_indices]\n",
    "            new_train_y_b_0 = train_data[i].y[train_size_a_0:train_size_a_0+train_size_b_0]\n",
    "\n",
    "        new_train_X = torch.concatenate([new_train_X_a_0, new_train_X_b_0, new_train_X_a_1, new_train_X_b_1])\n",
    "        new_train_y = torch.concatenate([new_train_y_a_0, new_train_y_b_0, new_train_y_a_1, new_train_y_b_1])\n",
    "        new_train_data.append(dataset(new_train_X, new_train_y, new_train_X.size()[0], 0, 0, 0, train_data[i].noise, torch.tensor([new_train_X_a_0.size()[0], new_train_X_b_0.size()[0], new_train_X_a_1.size()[0], new_train_X_b_1.size()[0]]).to(device), [], []))\n",
    "        # print(torch.tensor([new_train_X_a_0.size()[0], new_train_X_b_0.size()[0], new_train_X_a_1.size()[0], new_train_X_b_1.size()[0]]).to(device))\n",
    "    return new_train_data\n",
    "\n",
    "def mimic_bias_copy(train_data, num_candidate, test_ratio):\n",
    "    new_train_data = []\n",
    "    test_0_bias_ratio = test_ratio[0]/test_ratio[1]\n",
    "    test_1_bias_ratio = test_ratio[2]/test_ratio[3]\n",
    "    for i in range(num_candidate):\n",
    "        train_size_a_0, train_size_b_0, train_size_a_1, train_size_b_1 = train_data[i].bias\n",
    "        target_size_a_0 = int(train_size_b_0 * test_0_bias_ratio)\n",
    "        target_size_a_1 = int(train_size_b_1 * test_1_bias_ratio)\n",
    "\n",
    "        if train_size_a_0 < target_size_a_0:\n",
    "            num_extra = int(target_size_a_0/train_size_a_0) - 1\n",
    "            # extra_indices = torch.randint(0, train_size_a_0, (num_extra,))\n",
    "            indices = torch.range(0,train_size_a_0-1)\n",
    "            extra_indices = torch.tensor([])\n",
    "            for g in range(num_extra):\n",
    "                extra_indices = torch.cat([extra_indices, indices])\n",
    "            extra_indices = extra_indices.to(torch.long)\n",
    "            new_train_X_a_0 = torch.cat([train_data[i].X[:train_size_a_0], train_data[i].X[:train_size_a_0][extra_indices]])\n",
    "            new_train_X_b_0 = train_data[i].X[train_size_a_0:train_size_a_0+train_size_b_0]\n",
    "            new_train_y_a_0 = torch.cat([train_data[i].y[:train_size_a_0], train_data[i].y[:train_size_a_0][extra_indices]])\n",
    "            new_train_y_b_0 = train_data[i].y[train_size_a_0:train_size_a_0+train_size_b_0]\n",
    "        else:\n",
    "            target_size_b_0 = int(train_size_a_0 / test_0_bias_ratio)\n",
    "            num_extra = int(target_size_b_0/train_size_b_0) - 1\n",
    "            # extra_indices = torch.randint(0, train_size_b_0, (num_extra,))\n",
    "            indices = torch.range(0,train_size_b_0-1)\n",
    "            extra_indices = torch.tensor([])\n",
    "            for g in range(num_extra):\n",
    "                extra_indices = torch.cat([extra_indices, indices])\n",
    "            extra_indices = extra_indices.to(torch.long)\n",
    "            new_train_X_b_0 = torch.cat([train_data[i].X[train_size_a_0:train_size_a_0+train_size_b_0], train_data[i].X[train_size_a_0:train_size_a_0+train_size_b_0][extra_indices]])\n",
    "            new_train_X_a_0 = train_data[i].X[:train_size_a_0]\n",
    "            new_train_y_b_0 = torch.cat([train_data[i].y[train_size_a_0:train_size_a_0+train_size_b_0], train_data[i].y[train_size_a_0:train_size_a_0+train_size_b_0][extra_indices]])\n",
    "            new_train_y_a_0 = train_data[i].y[:train_size_a_0]\n",
    "\n",
    "        if train_size_a_1 < target_size_a_1:\n",
    "            num_extra = int(target_size_a_1/train_size_a_1) - 1\n",
    "            # extra_indices = torch.randint(0, train_size_a_1, (num_extra,))\n",
    "            indices = torch.range(0,train_size_a_1-1)\n",
    "            extra_indices = torch.tensor([])\n",
    "            for g in range(num_extra):\n",
    "                extra_indices = torch.cat([extra_indices, indices])\n",
    "            extra_indices = extra_indices.to(torch.long)\n",
    "            new_train_X_a_1 = torch.cat([train_data[i].X[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1], train_data[i].X[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1][extra_indices]])\n",
    "            new_train_X_b_1 = train_data[i].X[train_size_a_0+train_size_b_0+train_size_a_1:]\n",
    "            new_train_y_a_1 = torch.cat([train_data[i].y[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1], train_data[i].y[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1][extra_indices]])\n",
    "            new_train_y_b_1 = train_data[i].y[train_size_a_0+train_size_b_0+train_size_a_1:]\n",
    "        else:\n",
    "            target_size_b_1 = int(train_size_a_1 / test_1_bias_ratio)\n",
    "            num_extra = int(target_size_b_1/train_size_b_1) - 1\n",
    "            # extra_indices = torch.randint(0, train_size_b_1, (num_extra,))\n",
    "            indices = torch.range(0,train_size_b_1-1)\n",
    "            extra_indices = torch.tensor([])\n",
    "            for g in range(num_extra):\n",
    "                extra_indices = torch.cat([extra_indices, indices])\n",
    "            extra_indices = extra_indices.to(torch.long)\n",
    "            new_train_X_b_1 = torch.cat([train_data[i].X[train_size_a_0+train_size_b_0+train_size_a_1:], train_data[i].X[train_size_a_0+train_size_b_0+train_size_a_1:][extra_indices]])\n",
    "            new_train_X_a_1 = train_data[i].X[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1]\n",
    "            new_train_y_b_1 = torch.cat([train_data[i].y[train_size_a_0+train_size_b_0+train_size_a_1:], train_data[i].y[train_size_a_0+train_size_b_0+train_size_a_1:][extra_indices]])\n",
    "            new_train_y_a_1 = train_data[i].y[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1]\n",
    "\n",
    "        new_train_X = torch.concatenate([new_train_X_a_0, new_train_X_b_0, new_train_X_a_1, new_train_X_b_1])\n",
    "        new_train_y = torch.concatenate([new_train_y_a_0, new_train_y_b_0, new_train_y_a_1, new_train_y_b_1])\n",
    "        new_train_data.append(dataset(new_train_X, new_train_y, new_train_X.size()[0], 0, 0, 0, train_data[i].noise, torch.tensor([new_train_X_a_0.size()[0], new_train_X_b_0.size()[0], new_train_X_a_1.size()[0], new_train_X_b_1.size()[0]]).to(device), [], []))\n",
    "        # print(torch.tensor([new_train_X_a_0.size()[0], new_train_X_b_0.size()[0], new_train_X_a_1.size()[0], new_train_X_b_1.size()[0]]).to(device))\n",
    "    return new_train_data\n",
    "\n",
    "def mimic_bias_delete(train_data, num_candidate, test_ratio):\n",
    "    new_train_data = []\n",
    "    test_0_bias_ratio = test_ratio[0]/test_ratio[1]\n",
    "    test_1_bias_ratio = test_ratio[2]/test_ratio[3]\n",
    "    for i in range(num_candidate):\n",
    "        train_size_a_0, train_size_b_0, train_size_a_1, train_size_b_1 = train_data[i].bias\n",
    "        target_size_a_0 = int(train_size_b_0 * test_0_bias_ratio)\n",
    "        target_size_a_1 = int(train_size_b_1 * test_1_bias_ratio)\n",
    "\n",
    "        if train_size_a_0 > target_size_a_0:\n",
    "            residual_indices = torch.randperm(train_size_a_0)[:target_size_a_0]\n",
    "            new_train_X_a_0 = train_data[i].X[:train_size_a_0][residual_indices]\n",
    "            new_train_X_b_0 = train_data[i].X[train_size_a_0:train_size_a_0+train_size_b_0]\n",
    "            new_train_y_a_0 = train_data[i].y[:train_size_a_0][residual_indices]\n",
    "            new_train_y_b_0 = train_data[i].y[train_size_a_0:train_size_a_0+train_size_b_0]\n",
    "        else:\n",
    "            target_size_b_0 = int(train_size_a_0 / test_0_bias_ratio)\n",
    "            residual_indices = torch.randperm(train_size_b_0)[:target_size_b_0]\n",
    "            new_train_X_b_0 = train_data[i].X[train_size_a_0:train_size_a_0+train_size_b_0][residual_indices]\n",
    "            new_train_X_a_0 = train_data[i].X[:train_size_a_0]\n",
    "            new_train_y_b_0 = train_data[i].y[train_size_a_0:train_size_a_0+train_size_b_0][residual_indices]\n",
    "            new_train_y_a_0 = train_data[i].y[:train_size_a_0]\n",
    "\n",
    "        if train_size_a_1 > target_size_a_1:\n",
    "            residual_indices = torch.randperm(train_size_a_1)[:target_size_a_1]\n",
    "            new_train_X_a_1 = train_data[i].X[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1][residual_indices]\n",
    "            new_train_X_b_1 = train_data[i].X[train_size_a_0+train_size_b_0+train_size_a_1:]\n",
    "            new_train_y_a_1 = train_data[i].y[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1][residual_indices]\n",
    "            new_train_y_b_1 = train_data[i].y[train_size_a_0+train_size_b_0+train_size_a_1:]\n",
    "        else:\n",
    "            target_size_b_1 = int(train_size_a_1 / test_1_bias_ratio)\n",
    "            residual_indices = torch.randperm(train_size_b_1)[:target_size_b_1]\n",
    "            new_train_X_b_1 = train_data[i].X[train_size_a_0+train_size_b_0+train_size_a_1:][residual_indices]\n",
    "            new_train_X_a_1 = train_data[i].X[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1]\n",
    "            new_train_y_b_1 = train_data[i].y[train_size_a_0+train_size_b_0+train_size_a_1:][residual_indices]\n",
    "            new_train_y_a_1 = train_data[i].y[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1]\n",
    "\n",
    "        new_train_X = torch.concatenate([new_train_X_a_0, new_train_X_b_0, new_train_X_a_1, new_train_X_b_1])\n",
    "        new_train_y = torch.concatenate([new_train_y_a_0, new_train_y_b_0, new_train_y_a_1, new_train_y_b_1])\n",
    "        new_train_data.append(dataset(new_train_X, new_train_y, new_train_X.size()[0], 0, 0, 0, train_data[i].noise, torch.tensor([new_train_X_a_0.size()[0], new_train_X_b_0.size()[0], new_train_X_a_1.size()[0], new_train_X_b_1.size()[0]]).to(device), [], []))\n",
    "        # print(torch.tensor([new_train_X_a_0.size()[0], new_train_X_b_0.size()[0], new_train_X_a_1.size()[0], new_train_X_b_1.size()[0]]).to(device))\n",
    "    return new_train_data\n",
    "\n",
    "def data_denoise(train_data, num_candidate, ratio = 0.5):\n",
    "    new_train_data = []\n",
    "    for i in range(num_candidate):\n",
    "        train_size_a_0, train_size_b_0, train_size_a_1, train_size_b_1 = train_data[i].bias\n",
    "        noise = train_data[i].noise\n",
    "        new_train_X_a_0 = train_data[i].X[:train_size_a_0][int(ratio*noise*train_size_a_0):]\n",
    "        new_train_X_b_0 = train_data[i].X[train_size_a_0:train_size_a_0+train_size_b_0][int(ratio*noise*train_size_b_0):]\n",
    "        new_train_X_a_1 = train_data[i].X[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1][int(ratio*noise*train_size_a_1):]\n",
    "        new_train_X_b_1 = train_data[i].X[train_size_a_0+train_size_b_0+train_size_a_1:][int(ratio*noise*train_size_b_1):]\n",
    "        new_train_y_a_0 = train_data[i].y[:train_size_a_0][int(ratio*noise*train_size_a_0):]\n",
    "        new_train_y_b_0 = train_data[i].y[train_size_a_0:train_size_a_0+train_size_b_0][int(ratio*noise*train_size_b_0):]\n",
    "        new_train_y_a_1 = train_data[i].y[train_size_a_0+train_size_b_0:train_size_a_0+train_size_b_0+train_size_a_1][int(ratio*noise*train_size_a_1):]\n",
    "        new_train_y_b_1 = train_data[i].y[train_size_a_0+train_size_b_0+train_size_a_1:][int(ratio*noise*train_size_b_1):]\n",
    "        new_train_X = torch.concatenate([new_train_X_a_0, new_train_X_b_0, new_train_X_a_1, new_train_X_b_1])\n",
    "        new_train_y = torch.concatenate([new_train_y_a_0, new_train_y_b_0, new_train_y_a_1, new_train_y_b_1])\n",
    "        new_train_data.append(dataset(new_train_X, new_train_y, new_train_X.size()[0], 0, 0, 0, train_data[i].noise*(1-ratio), torch.tensor([new_train_X_a_0.size()[0], new_train_X_b_0.size()[0], new_train_X_a_1.size()[0], new_train_X_b_1.size()[0]]).to(device), [], []))\n",
    "        # print(torch.tensor([new_train_X_a_0.size()[0], new_train_X_b_0.size()[0], new_train_X_a_1.size()[0], new_train_X_b_1.size()[0]]).to(device))\n",
    "    return new_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:24<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original score:  9.0650 , mimic label copy:  0.7016 , mimic label delete:  -2.0716 , mimic bias copy:  0.7016 , mimic bias delete:  -2.0323 , data denoise:  0.1374\n",
      "original loss:  0.4296 , mimic label copy:  -0.0171 , mimic label delete:  -0.0234 , mimic bias copy:  -0.0171 , mimic bias delete:  -0.0272 , data denoise:  -0.0385\n",
      "original acc:  0.8078 , mimic label copy:  0.0188 , mimic label delete:  0.0104 , mimic bias copy:  0.0188 , mimic bias delete:  0.0118 , data denoise:  0.0197\n",
      "& 5\\% & PMI & 9.0650 & 0.7016 & -2.0716 & 0.7016 & -2.0323 & 0.1374 \\\\\n",
      "& & Loss & 0.4296 & -0.0171 & -0.0234 & -0.0171 & -0.0272 & -0.0385 \\\\\n",
      "& & Acc & 0.8078 & 0.0188 & 0.0104 & 0.0188 & 0.0118 & 0.0197 \\\\\n",
      "original loss for cluster a0:  0.4579 , mimic label copy:  -0.0315 , mimic label delete:  -0.0975 , mimic bias copy:  -0.0315 , mimic bias delete:  -0.1077 , data denoise:  -0.0408\n",
      "original loss for cluster b0:  0.2469 , mimic label copy:  0.0662 , mimic label delete:  0.2104 , mimic bias copy:  0.0662 , mimic bias delete:  0.2088 , data denoise:  -0.0534\n",
      "original loss for cluster a1:  0.2384 , mimic label copy:  0.0821 , mimic label delete:  0.2606 , mimic bias copy:  0.0821 , mimic bias delete:  0.2702 , data denoise:  -0.0556\n",
      "original loss for cluster b1:  0.5884 , mimic label copy:  -0.0939 , mimic label delete:  -0.2083 , mimic bias copy:  -0.0939 , mimic bias delete:  -0.2134 , data denoise:  -0.0200\n",
      "original acc for cluster a0:  0.8015 , mimic label copy:  0.0260 , mimic label delete:  0.0423 , mimic bias copy:  0.0260 , mimic bias delete:  0.0495 , data denoise:  0.0220\n",
      "original acc for cluster b0:  0.9065 , mimic label copy:  -0.0305 , mimic label delete:  -0.1055 , mimic bias copy:  -0.0305 , mimic bias delete:  -0.1110 , data denoise:  0.0240\n",
      "original acc for cluster a1:  0.9060 , mimic label copy:  -0.0360 , mimic label delete:  -0.1455 , mimic bias copy:  -0.0360 , mimic bias delete:  -0.1425 , data denoise:  0.0200\n",
      "original acc for cluster b1:  0.7155 , mimic label copy:  0.0638 , mimic label delete:  0.1145 , mimic bias copy:  0.0638 , mimic bias delete:  0.1127 , data denoise:  0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "T = 1\n",
    "D = 100\n",
    "train_size_a_0 = 20\n",
    "train_size_a_1 = 40\n",
    "train_size_b_0 = 40\n",
    "train_size_b_1 = 20\n",
    "num_candidate = 1\n",
    "test_size_a_0 = 40\n",
    "test_size_a_1 = 20\n",
    "test_size_b_0 = 20\n",
    "test_size_b_1 = 40\n",
    "noise_levels = [5]\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "test_label_ratio = (test_size_a_0 + test_size_b_0)/(test_size_a_1 + test_size_b_1)\n",
    "test_bias_ratio = (test_size_a_0 + test_size_a_1)/(test_size_b_0 + test_size_b_1)\n",
    "test_size = test_size_a_0 + test_size_a_1 + test_size_b_0 + test_size_b_1\n",
    "test_ratio = (test_size_a_0, test_size_b_0, test_size_a_1, test_size_b_1)\n",
    "for noise_level in noise_levels:\n",
    "    pre_score = 0\n",
    "    mimic_label_copy_score = 0\n",
    "    mimic_label_delete_socre = 0\n",
    "    mimic_bias_copy_score = 0\n",
    "    mimic_bias_delete_score = 0\n",
    "    data_denoise_score = 0\n",
    "    pre_loss = 0\n",
    "    mimic_label_copy_loss = 0\n",
    "    mimic_label_delete_loss = 0\n",
    "    mimic_bias_copy_loss = 0\n",
    "    mimic_bias_delete_loss = 0\n",
    "    data_denoise_loss = 0\n",
    "    pre_acc = 0\n",
    "    mimic_label_copy_acc = 0\n",
    "    mimic_label_delete_acc = 0\n",
    "    mimic_bias_copy_acc = 0\n",
    "    mimic_bias_delete_acc = 0\n",
    "    data_denoise_acc = 0\n",
    "    pre_bias_loss = np.array([0.,0.,0.,0.])\n",
    "    mimic_label_copy_bias_loss = np.array([0.,0.,0.,0.])\n",
    "    mimic_label_delete_bias_loss = np.array([0.,0.,0.,0.])\n",
    "    mimic_bias_copy_bias_loss = np.array([0.,0.,0.,0.])\n",
    "    mimic_bias_delete_bias_loss = np.array([0.,0.,0.,0.])\n",
    "    data_denoise_bias_loss = np.array([0.,0.,0.,0.])\n",
    "    pre_bias_acc = np.array([0.,0.,0.,0.])\n",
    "    mimic_label_copy_bias_acc = np.array([0.,0.,0.,0.])\n",
    "    mimic_label_delete_bias_acc = np.array([0.,0.,0.,0.])\n",
    "    mimic_bias_copy_bias_acc = np.array([0.,0.,0.,0.])\n",
    "    mimic_bias_delete_bias_acc = np.array([0.,0.,0.,0.])\n",
    "    data_denoise_bias_acc = np.array([0.,0.,0.,0.])\n",
    "    for d in tqdm(range(D)):\n",
    "        test_X = P_x\n",
    "        test_y = P_y\n",
    "        # penalty = 10000\n",
    "    #     best_penalty = 1\n",
    "    #     best_loss = 100\n",
    "    #     for c in range(1, 11):\n",
    "    #         loss = 0\n",
    "    #         for i in range(30):\n",
    "    #             sub_test_X, sub_test_y = subsample(Q_x, Q_y, 200)\n",
    "    #             test = LogisticRegression(fit_intercept = False, C = c, max_iter=50000).fit(sub_test_X.cpu(), sub_test_y.cpu())\n",
    "    #             # acc += test.score(test_X.cpu(), test_y.cpu())\n",
    "    #             mu = torch.tensor(test.coef_, dtype=torch.float32, device=device)\n",
    "    #             predictive = sigmoid(torch.matmul(test_X, mu.t())).squeeze()\n",
    "    #             predictions = (predictive >= 0.5).float()\n",
    "    #             loss += criterion(predictive, test_y)\n",
    "    #         if loss < best_loss:\n",
    "    #             penalty = c\n",
    "    #             best_loss = loss\n",
    "    #         print(loss)\n",
    "    #     print(penalty)\n",
    "    #     for t in range(30):\n",
    "    #         loss_1 = 0\n",
    "    #         loss_2 = 0\n",
    "    #         for i in range(30):\n",
    "    #             sub_test_X, sub_test_y = subsample(Q_x, Q_y, 200)\n",
    "    #             test = LogisticRegression(fit_intercept = False, C = penalty+0.1, max_iter=50000).fit(sub_test_X.cpu(), sub_test_y.cpu())\n",
    "    #             # acc_1 += test.score(test_X.cpu(), test_y.cpu())\n",
    "    #             mu = torch.tensor(test.coef_, dtype=torch.float32, device=device)\n",
    "    #             predictive = sigmoid(torch.matmul(test_X, mu.t())).squeeze()\n",
    "    #             predictions = (predictive >= 0.5).float()\n",
    "    #             loss_1 += criterion(predictive, test_y)\n",
    "    #         for i in range(30):\n",
    "    #             sub_test_X, sub_test_y = subsample(Q_x, Q_y, 200)\n",
    "    #             test = LogisticRegression(fit_intercept = False, C = penalty-0.1, max_iter=50000).fit(sub_test_X.cpu(), sub_test_y.cpu())\n",
    "    #             # acc_2 += test.score(test_X.cpu(), test_y.cpu())\n",
    "    #             mu = torch.tensor(test.coef_, dtype=torch.float32, device=device)\n",
    "    #             predictive = sigmoid(torch.matmul(test_X, mu.t())).squeeze()\n",
    "    #             predictions = (predictive >= 0.5).float()\n",
    "    #             loss_2 += criterion(predictive, test_y)\n",
    "    #         # test = LogisticRegression(fit_intercept = False, C = penalty, max_iter=50000).fit(Q_x.cpu(), Q_y.cpu())\n",
    "    #         # acc_1 = test.score(test_X.cpu(), test_y.cpu())\n",
    "    #         # test = LogisticRegression(fit_intercept = False, C = penalty-0.1, max_iter=50000).fit(Q_x.cpu(), Q_y.cpu())\n",
    "    #         # acc_2 = test.score(test_X.cpu(), test_y.cpu())\n",
    "    #         if loss_1 < best_loss:\n",
    "    #             best_loss = loss_1\n",
    "    #             best_penalty = penalty+0.1\n",
    "    #         if loss_2 < best_loss:\n",
    "    #             best_loss = loss_2\n",
    "    #             best_penalty = penalty-0.1\n",
    "    #         penalty = penalty - (loss_1.item()-loss_2.item())/8\n",
    "    #         print(loss_1, loss_2, penalty)\n",
    "    #     # print(acc)\n",
    "    #     print(best_penalty, best_loss, penalty)\n",
    "    #     sys.exit()\n",
    "\n",
    "        train_data = generate_train_cifar10(train_size_a_0, train_size_a_1, train_size_b_0, train_size_b_1, num_candidate, noise_level)\n",
    "        mimic_label_copy_train_data = mimic_label_copy(train_data, num_candidate, test_ratio)\n",
    "        mimic_label_delete_train_data = mimic_label_delete(train_data, num_candidate, test_ratio)\n",
    "        mimic_bias_copy_train_data = mimic_bias_copy(train_data, num_candidate, test_ratio)\n",
    "        mimic_bias_delete_train_data = mimic_bias_delete(train_data, num_candidate, test_ratio)\n",
    "        data_denoise_train_data = data_denoise(train_data, num_candidate, ratio=1)\n",
    "\n",
    "        # for t in range(T):\n",
    "        sample_test_X_a_0, sample_test_y_a_0 = subsample(images_a_0_embedding[4000:], torch.zeros(1000).to(device), test_size_a_0)\n",
    "        sample_test_X_a_1, sample_test_y_a_1 = subsample(images_a_1_embedding[4000:], torch.ones(1000).to(device), test_size_a_1)\n",
    "        sample_test_X_b_0, sample_test_y_b_0 = subsample(images_b_0_embedding[4000:], torch.zeros(1000).to(device), test_size_b_0)\n",
    "        sample_test_X_b_1, sample_test_y_b_1 = subsample(images_b_1_embedding[4000:], torch.ones(1000).to(device), test_size_b_1)\n",
    "        sample_test_X = torch.concatenate([sample_test_X_a_0, sample_test_X_b_0, sample_test_X_a_1, sample_test_X_b_1])\n",
    "        sample_test_y = torch.concatenate([sample_test_y_a_0, sample_test_y_b_0, sample_test_y_a_1, sample_test_y_b_1])\n",
    "\n",
    "        get_err_score(train_data, sample_test_X, sample_test_y, num_candidate, test_ratio)\n",
    "        get_err_score(mimic_label_copy_train_data, sample_test_X, sample_test_y, num_candidate, test_ratio)\n",
    "        get_err_score(mimic_label_delete_train_data, sample_test_X, sample_test_y, num_candidate, test_ratio)\n",
    "        get_err_score(mimic_bias_copy_train_data, sample_test_X, sample_test_y, num_candidate, test_ratio)\n",
    "        get_err_score(mimic_bias_delete_train_data, sample_test_X, sample_test_y, num_candidate, test_ratio)\n",
    "        get_err_score(data_denoise_train_data, sample_test_X, sample_test_y, num_candidate, test_ratio)\n",
    "\n",
    "        for i in range(num_candidate):\n",
    "            pre_score += train_data[i].score\n",
    "            mimic_label_copy_score += mimic_label_copy_train_data[i].score\n",
    "            mimic_label_delete_socre += mimic_label_delete_train_data[i].score\n",
    "            mimic_bias_copy_score += mimic_bias_copy_train_data[i].score\n",
    "            mimic_bias_delete_score += mimic_bias_delete_train_data[i].score\n",
    "            data_denoise_score += data_denoise_train_data[i].score\n",
    "            pre_loss += train_data[i].base_loss\n",
    "            mimic_label_copy_loss += mimic_label_copy_train_data[i].base_loss\n",
    "            mimic_label_delete_loss += mimic_label_delete_train_data[i].base_loss\n",
    "            mimic_bias_copy_loss += mimic_bias_copy_train_data[i].base_loss\n",
    "            mimic_bias_delete_loss += mimic_bias_delete_train_data[i].base_loss\n",
    "            data_denoise_loss += data_denoise_train_data[i].base_loss\n",
    "            pre_acc += train_data[i].base_acc\n",
    "            mimic_label_copy_acc += mimic_label_copy_train_data[i].base_acc\n",
    "            mimic_label_delete_acc += mimic_label_delete_train_data[i].base_acc\n",
    "            mimic_bias_copy_acc += mimic_bias_copy_train_data[i].base_acc\n",
    "            mimic_bias_delete_acc += mimic_bias_delete_train_data[i].base_acc\n",
    "            data_denoise_acc += data_denoise_train_data[i].base_acc\n",
    "            pre_bias_loss += train_data[i].bias_loss\n",
    "            mimic_label_copy_bias_loss += mimic_label_copy_train_data[i].bias_loss\n",
    "            mimic_label_delete_bias_loss += mimic_label_delete_train_data[i].bias_loss\n",
    "            mimic_bias_copy_bias_loss += mimic_bias_copy_train_data[i].bias_loss\n",
    "            mimic_bias_delete_bias_loss += mimic_bias_delete_train_data[i].bias_loss\n",
    "            data_denoise_bias_loss += data_denoise_train_data[i].bias_loss\n",
    "            pre_bias_acc += train_data[i].bias_acc\n",
    "            mimic_label_copy_bias_acc += mimic_label_copy_train_data[i].bias_acc\n",
    "            mimic_label_delete_bias_acc += mimic_label_delete_train_data[i].bias_acc\n",
    "            mimic_bias_copy_bias_acc += mimic_bias_copy_train_data[i].bias_acc\n",
    "            mimic_bias_delete_bias_acc += mimic_bias_delete_train_data[i].bias_acc\n",
    "            data_denoise_bias_acc += data_denoise_train_data[i].bias_acc\n",
    "    pre_score /= (T*D*num_candidate)\n",
    "    mimic_label_copy_score /= (T*D*num_candidate)\n",
    "    mimic_label_delete_socre /= (T*D*num_candidate)\n",
    "    mimic_bias_copy_score /= (T*D*num_candidate)\n",
    "    mimic_bias_delete_score /= (T*D*num_candidate)\n",
    "    data_denoise_score /= (T*D*num_candidate)\n",
    "    pre_loss /= (T*D*num_candidate)\n",
    "    mimic_label_copy_loss /= (T*D*num_candidate)\n",
    "    mimic_label_delete_loss /= (T*D*num_candidate)\n",
    "    mimic_bias_copy_loss /= (T*D*num_candidate)\n",
    "    mimic_bias_delete_loss /= (T*D*num_candidate)\n",
    "    data_denoise_loss /= (T*D*num_candidate)\n",
    "    pre_acc /= (T*D*num_candidate)\n",
    "    mimic_label_copy_acc /= (T*D*num_candidate)\n",
    "    mimic_label_delete_acc /= (T*D*num_candidate)\n",
    "    mimic_bias_copy_acc /= (T*D*num_candidate)\n",
    "    mimic_bias_delete_acc /= (T*D*num_candidate)\n",
    "    data_denoise_acc /= (T*D*num_candidate)\n",
    "    pre_bias_loss /= (T*D*num_candidate)\n",
    "    mimic_label_copy_bias_loss /= (T*D*num_candidate)\n",
    "    mimic_label_delete_bias_loss /= (T*D*num_candidate)\n",
    "    mimic_bias_copy_bias_loss /= (T*D*num_candidate)\n",
    "    mimic_bias_delete_bias_loss /= (T*D*num_candidate)\n",
    "    data_denoise_bias_loss /= (T*D*num_candidate)\n",
    "    pre_bias_acc /= (T*D*num_candidate)\n",
    "    mimic_label_copy_bias_acc /= (T*D*num_candidate)\n",
    "    mimic_label_delete_bias_acc /= (T*D*num_candidate)\n",
    "    mimic_bias_copy_bias_acc /= (T*D*num_candidate)\n",
    "    mimic_bias_delete_bias_acc /= (T*D*num_candidate)\n",
    "    data_denoise_bias_acc /= (T*D*num_candidate)\n",
    "    print(\"original score: \", '%.4f'%pre_score, \", mimic label copy: \", '%.4f'%(mimic_label_copy_score - pre_score), \", mimic label delete: \", '%.4f'%(mimic_label_delete_socre - pre_score), \", mimic bias copy: \", '%.4f'%(mimic_bias_copy_score - pre_score), \", mimic bias delete: \", '%.4f'%(mimic_bias_delete_score - pre_score), \", data denoise: \", '%.4f'%(data_denoise_score - pre_score))\n",
    "    print(\"original loss: \", '%.4f'%pre_loss, \", mimic label copy: \", '%.4f'%(mimic_label_copy_loss - pre_loss), \", mimic label delete: \", '%.4f'%(mimic_label_delete_loss - pre_loss), \", mimic bias copy: \", '%.4f'%(mimic_bias_copy_loss - pre_loss), \", mimic bias delete: \", '%.4f'%(mimic_bias_delete_loss - pre_loss), \", data denoise: \", '%.4f'%(data_denoise_loss - pre_loss))\n",
    "    print(\"original acc: \", '%.4f'%pre_acc, \", mimic label copy: \", '%.4f'%(mimic_label_copy_acc - pre_acc), \", mimic label delete: \", '%.4f'%(mimic_label_delete_acc - pre_acc), \", mimic bias copy: \", '%.4f'%(mimic_bias_copy_acc - pre_acc), \", mimic bias delete: \", '%.4f'%(mimic_bias_delete_acc - pre_acc), \", data denoise: \", '%.4f'%(data_denoise_acc - pre_acc))\n",
    "    print(\"&\", \"{}\\\\%\".format(noise_level),\"&\", \"PMI\", \"&\", '%.4f'%pre_score, \"&\", '%.4f'%(mimic_label_copy_score - pre_score), \"&\", '%.4f'%(mimic_label_delete_socre - pre_score), \"&\", '%.4f'%(mimic_bias_copy_score - pre_score), \"&\", '%.4f'%(mimic_bias_delete_score - pre_score), \"&\", '%.4f'%(data_denoise_score - pre_score), \"\\\\\\\\\")\n",
    "    print(\"&\", \"&\", \"Loss\", \"&\", '%.4f'%pre_loss, \"&\", '%.4f'%(mimic_label_copy_loss - pre_loss), \"&\", '%.4f'%(mimic_label_delete_loss - pre_loss), \"&\", '%.4f'%(mimic_bias_copy_loss - pre_loss), \"&\", '%.4f'%(mimic_bias_delete_loss - pre_loss), \"&\", '%.4f'%(data_denoise_loss - pre_loss), \"\\\\\\\\\")\n",
    "    print(\"&\", \"&\", \"Acc\", \"&\", '%.4f'%pre_acc, \"&\", '%.4f'%(mimic_label_copy_acc - pre_acc), \"&\", '%.4f'%(mimic_label_delete_acc - pre_acc), \"&\", '%.4f'%(mimic_bias_copy_acc - pre_acc), \"&\", '%.4f'%(mimic_bias_delete_acc - pre_acc), \"&\", '%.4f'%(data_denoise_acc - pre_acc), \"\\\\\\\\\")\n",
    "    print(\"original loss for cluster a0: \", '%.4f'%pre_bias_loss[0], \", mimic label copy: \", '%.4f'%(mimic_label_copy_bias_loss[0] - pre_bias_loss[0]), \", mimic label delete: \", '%.4f'%(mimic_label_delete_bias_loss[0] - pre_bias_loss[0]), \", mimic bias copy: \", '%.4f'%(mimic_bias_copy_bias_loss[0] - pre_bias_loss[0]), \", mimic bias delete: \", '%.4f'%(mimic_bias_delete_bias_loss[0] - pre_bias_loss[0]), \", data denoise: \", '%.4f'%(data_denoise_bias_loss[0] - pre_bias_loss[0]))\n",
    "    print(\"original loss for cluster b0: \", '%.4f'%pre_bias_loss[1], \", mimic label copy: \", '%.4f'%(mimic_label_copy_bias_loss[1] - pre_bias_loss[1]), \", mimic label delete: \", '%.4f'%(mimic_label_delete_bias_loss[1] - pre_bias_loss[1]), \", mimic bias copy: \", '%.4f'%(mimic_bias_copy_bias_loss[1] - pre_bias_loss[1]), \", mimic bias delete: \", '%.4f'%(mimic_bias_delete_bias_loss[1] - pre_bias_loss[1]), \", data denoise: \", '%.4f'%(data_denoise_bias_loss[1] - pre_bias_loss[1]))\n",
    "    print(\"original loss for cluster a1: \", '%.4f'%pre_bias_loss[2], \", mimic label copy: \", '%.4f'%(mimic_label_copy_bias_loss[2] - pre_bias_loss[2]), \", mimic label delete: \", '%.4f'%(mimic_label_delete_bias_loss[2] - pre_bias_loss[2]), \", mimic bias copy: \", '%.4f'%(mimic_bias_copy_bias_loss[2] - pre_bias_loss[2]), \", mimic bias delete: \", '%.4f'%(mimic_bias_delete_bias_loss[2] - pre_bias_loss[2]), \", data denoise: \", '%.4f'%(data_denoise_bias_loss[2] - pre_bias_loss[2]))\n",
    "    print(\"original loss for cluster b1: \", '%.4f'%pre_bias_loss[3], \", mimic label copy: \", '%.4f'%(mimic_label_copy_bias_loss[3] - pre_bias_loss[3]), \", mimic label delete: \", '%.4f'%(mimic_label_delete_bias_loss[3] - pre_bias_loss[3]), \", mimic bias copy: \", '%.4f'%(mimic_bias_copy_bias_loss[3] - pre_bias_loss[3]), \", mimic bias delete: \", '%.4f'%(mimic_bias_delete_bias_loss[3] - pre_bias_loss[3]), \", data denoise: \", '%.4f'%(data_denoise_bias_loss[3] - pre_bias_loss[3]))\n",
    "    print(\"original acc for cluster a0: \", '%.4f'%pre_bias_acc[0], \", mimic label copy: \", '%.4f'%(mimic_label_copy_bias_acc[0] - pre_bias_acc[0]), \", mimic label delete: \", '%.4f'%(mimic_label_delete_bias_acc[0] - pre_bias_acc[0]), \", mimic bias copy: \", '%.4f'%(mimic_bias_copy_bias_acc[0] - pre_bias_acc[0]), \", mimic bias delete: \", '%.4f'%(mimic_bias_delete_bias_acc[0] - pre_bias_acc[0]), \", data denoise: \", '%.4f'%(data_denoise_bias_acc[0] - pre_bias_acc[0]))\n",
    "    print(\"original acc for cluster b0: \", '%.4f'%pre_bias_acc[1], \", mimic label copy: \", '%.4f'%(mimic_label_copy_bias_acc[1] - pre_bias_acc[1]), \", mimic label delete: \", '%.4f'%(mimic_label_delete_bias_acc[1] - pre_bias_acc[1]), \", mimic bias copy: \", '%.4f'%(mimic_bias_copy_bias_acc[1] - pre_bias_acc[1]), \", mimic bias delete: \", '%.4f'%(mimic_bias_delete_bias_acc[1] - pre_bias_acc[1]), \", data denoise: \", '%.4f'%(data_denoise_bias_acc[1] - pre_bias_acc[1]))\n",
    "    print(\"original acc for cluster a1: \", '%.4f'%pre_bias_acc[2], \", mimic label copy: \", '%.4f'%(mimic_label_copy_bias_acc[2] - pre_bias_acc[2]), \", mimic label delete: \", '%.4f'%(mimic_label_delete_bias_acc[2] - pre_bias_acc[2]), \", mimic bias copy: \", '%.4f'%(mimic_bias_copy_bias_acc[2] - pre_bias_acc[2]), \", mimic bias delete: \", '%.4f'%(mimic_bias_delete_bias_acc[2] - pre_bias_acc[2]), \", data denoise: \", '%.4f'%(data_denoise_bias_acc[2] - pre_bias_acc[2]))\n",
    "    print(\"original acc for cluster b1: \", '%.4f'%pre_bias_acc[3], \", mimic label copy: \", '%.4f'%(mimic_label_copy_bias_acc[3] - pre_bias_acc[3]), \", mimic label delete: \", '%.4f'%(mimic_label_delete_bias_acc[3] - pre_bias_acc[3]), \", mimic bias copy: \", '%.4f'%(mimic_bias_copy_bias_acc[3] - pre_bias_acc[3]), \", mimic bias delete: \", '%.4f'%(mimic_bias_delete_bias_acc[3] - pre_bias_acc[3]), \", data denoise: \", '%.4f'%(data_denoise_bias_acc[3] - pre_bias_acc[3]))\n",
    "            # for i in range(num_candidate):\n",
    "            #     print(\"original score: \", train_data[i].score, \", change of score: \", new_train_data[i].score - train_data[i].score)\n",
    "            #     print(\"original base loss: \", train_data[i].base_loss, \", change of base loss: \", new_train_data[i].base_loss - train_data[i].base_loss)\n",
    "            #     # print(\"original post loss: \", train_data[i].post_loss, \", change of post loss: \", new_train_data[i].post_loss - train_data[i].post_loss)\n",
    "            #     # print(\"original smooth loss: \", train_data[i].smooth, \", change of smooth loss: \", new_train_data[i].smooth - train_data[i].smooth)5\n",
    "            #     print(\"original base acc: \", train_data[i].base_acc, \", change of base acc: \", new_train_data[i].base_acc - train_data[i].base_acc)\n",
    "            #     # print(\"original post acc: \", train_data[i].post_acc, \", change of post acc: \", new_train_data[i].post_acc - train_data[i].post_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_test_X_a_0, sample_test_y_a_0 = images_a_0_embedding[4000:4400], torch.zeros(400).to(device)\n",
    "# sample_test_X_a_1, sample_test_y_a_1 = images_a_1_embedding[4000:4400], torch.ones(400).to(device)\n",
    "# sample_test_X_b_0, sample_test_y_b_0 = images_b_0_embedding[4000:4400], torch.zeros(400).to(device)\n",
    "# sample_test_X_b_1, sample_test_y_b_1 = images_b_1_embedding[4000:4400], torch.ones(400).to(device)\n",
    "# sample_test_X = torch.concatenate([sample_test_X_a_0, sample_test_X_a_1, sample_test_X_b_0, sample_test_X_b_1])\n",
    "# sample_test_y = torch.concatenate([sample_test_y_a_0, sample_test_y_a_1, sample_test_y_b_0, sample_test_y_b_1])\n",
    "\n",
    "# test = LogisticRegression(fit_intercept = False, C = penalty, max_iter=5000).fit(sample_test_X.cpu(), sample_test_y.cpu())\n",
    "# print(test.score(P_x.cpu(), P_y.cpu()))\n",
    "# mu_test = torch.tensor(test.coef_, dtype=torch.float32, device=device)\n",
    "# Q_test = compute_hessian(mu_test, sample_test_X)\n",
    "\n",
    "# L = torch.linalg.cholesky(Q_test)\n",
    "# lg2 = 2 * torch.sum(torch.log(torch.diagonal(L)))\n",
    "# train_data_X = torch.concatenate([\n",
    "#     images_a_0_embedding[:1600],\n",
    "#     images_b_0_embedding[:400],\n",
    "#     images_a_1_embedding[:1600],\n",
    "#     images_b_1_embedding[:400],\n",
    "#     ])\n",
    "# train_data_y = torch.concatenate([torch.zeros(1440), torch.ones(160), torch.zeros(360), torch.ones(40+1440), torch.zeros(160), torch.ones(360), torch.zeros(40)]).to(device)\n",
    "# # train_data_y = torch.concatenate([torch.zeros(2000), torch.ones(2000)]).to(device)\n",
    "# print(test.score(train_data_X.cpu(), train_data_y.cpu()))\n",
    "# score, _, _ = compute_data_score_err(mu_test, Q_test, sample_test_X, sample_test_y, train_data_X, train_data_y, lg2)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_test_X_a_0, sample_test_y_a_0 = subsample(images_a_0_embedding[4000:], torch.zeros(1000).to(device), 80)\n",
    "# sample_test_X_a_1, sample_test_y_a_1 = subsample(images_a_1_embedding[4000:], torch.ones(1000).to(device), 80)\n",
    "# sample_test_X_b_0, sample_test_y_b_0 = subsample(images_b_0_embedding[4000:], torch.zeros(1000).to(device), 80)\n",
    "# sample_test_X_b_1, sample_test_y_b_1 = subsample(images_b_1_embedding[4000:], torch.ones(1000).to(device), 80)\n",
    "# sample_test_X = torch.concatenate([sample_test_X_a_0, sample_test_X_a_1, sample_test_X_b_0, sample_test_X_b_1])\n",
    "# sample_test_y = torch.concatenate([sample_test_y_a_0, sample_test_y_a_1, sample_test_y_b_0, sample_test_y_b_1])\n",
    "\n",
    "# test = LogisticRegression(fit_intercept = False, C = penalty, max_iter=5000).fit(sample_test_X.cpu(), sample_test_y.cpu())\n",
    "# mu_test = torch.tensor(test.coef_, dtype=torch.float32, device=device)\n",
    "# Q_test = compute_hessian(mu_test, sample_test_X)\n",
    "\n",
    "# L = torch.linalg.cholesky(Q_test)\n",
    "# lg2 = 2 * torch.sum(torch.log(torch.diagonal(L)))\n",
    "# scores = []\n",
    "# for i in tqdm(range(1000)):\n",
    "#     train_data_X = torch.concatenate([\n",
    "#     images_a_0_embedding[:i+1],\n",
    "#     images_b_0_embedding[:i+1],\n",
    "#     images_a_1_embedding[:i+1],\n",
    "#     images_b_1_embedding[:i+1],\n",
    "#     ])\n",
    "#     train_data_y = torch.concatenate([torch.zeros(2*i+2), torch.ones(2*i+2)]).to(device)\n",
    "#     score, _, _ = compute_data_score_err(mu_test, Q_test, sample_test_X, sample_test_y, train_data_X, train_data_y, lg2)\n",
    "#     scores.append(score)\n",
    "# # Prepare X-axis values (4 * i)\n",
    "# x_values = [4 * i for i in range(1000)]\n",
    "\n",
    "# # Plot scores\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(x_values, scores, marker='o', linestyle='-', color='b', label='Score')\n",
    "# plt.xlabel('Training Data Size')\n",
    "# plt.ylabel('Score')\n",
    "# plt.title('Scores vs. Training Data Size')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from pyinstrument import Profiler\n",
    "\n",
    "# # profiler = Profiler()\n",
    "# # profiler.start()\n",
    "# x_label_all = []\n",
    "\n",
    "# single_smooth_post_L_all = []\n",
    "# single_smooth_post_H_all = []\n",
    "# single_score_post_L_all = []\n",
    "# single_score_post_H_all = []\n",
    "# single_post_base_loss_L_all = []\n",
    "# single_post_base_loss_H_all = []\n",
    "# single_post_base_acc_L_all = []\n",
    "# single_post_base_acc_H_all = []\n",
    "\n",
    "# smooth_post_L_all = []\n",
    "# smooth_post_H_all = []\n",
    "# score_post_L_all = []\n",
    "# score_post_H_all = []\n",
    "# post_base_loss_L_all = []\n",
    "# post_base_loss_H_all = []\n",
    "# post_base_acc_L_all = []\n",
    "# post_base_acc_H_all = []\n",
    "\n",
    "# single_acc_all_all = []\n",
    "# acc_all_all = []\n",
    "# single_loss_all_all = []\n",
    "# loss_all_all = []\n",
    "# noise_ratio_all_all = []\n",
    "# label_mae_all_all = []\n",
    "# bias_mae_all_all = []\n",
    "\n",
    "# for test_size in test_size_num:\n",
    "#     x_label = []\n",
    "\n",
    "#     single_smooth_post_L = []\n",
    "#     single_smooth_post_H = []\n",
    "#     single_score_post_L = []\n",
    "#     single_score_post_H = []\n",
    "#     single_post_base_loss_L = []\n",
    "#     single_post_base_loss_H = []\n",
    "#     single_post_base_acc_L = []\n",
    "#     single_post_base_acc_H = []\n",
    "\n",
    "#     smooth_post_L = []\n",
    "#     smooth_post_H = []\n",
    "#     score_post_L = []\n",
    "#     score_post_H = []\n",
    "#     post_base_loss_L = []\n",
    "#     post_base_loss_H = []\n",
    "#     post_base_acc_L = []\n",
    "#     post_base_acc_H = []\n",
    "\n",
    "#     single_acc_all = []\n",
    "#     acc_all = []\n",
    "#     single_loss_all = []\n",
    "#     loss_all = []\n",
    "#     noise_ratio_all = []\n",
    "#     label_mae_all = []\n",
    "#     bias_mae_all = []\n",
    "\n",
    "#     # for num_candidate in range(candidate_L, candidate_H+1, candidate_step):\n",
    "#     for train_size in train_size_num:\n",
    "#         single_total_summary = []\n",
    "#         total_summary = []\n",
    "#         total_noise_ratio = []\n",
    "#         total_label_mae = []\n",
    "#         total_bias_mae = []\n",
    "#         for i in range(len(top_candidate)):\n",
    "#             single_total_summary.append(summary([0,0], [0,0], [0,0], [0,0], result(0,0,0,0,0,0,0,0,0,0,0,0)))\n",
    "#             total_summary.append(summary([0,0], [0,0], [0,0], [0,0], result(0,0,0,0,0,0,0,0,0,0,0,0)))\n",
    "#             total_noise_ratio.append(np.zeros(6))\n",
    "#             total_label_mae.append(np.zeros(6))\n",
    "#             total_bias_mae.append(np.zeros(6))\n",
    "#         for d in tqdm(range(D)):\n",
    "#             test_X = P_x\n",
    "#             test_y = P_y\n",
    "#             # test = LogisticRegression(fit_intercept = False, C = penalty, max_iter=50000).fit(test_X.cpu(), test_y.cpu())\n",
    "#             # acc = test.score(test_X.cpu(), test_y.cpu())\n",
    "#             # print(acc)\n",
    "#             # sys.exit()\n",
    "#             num_candidate = candidate_num\n",
    "#             train_data = generate_train_cifar10(train_size, num_candidate)\n",
    "#             single_total_results = []\n",
    "#             total_results = []\n",
    "#             single_results = []\n",
    "#             results = []\n",
    "#             for i in range(len(top_candidate)):\n",
    "#                 single_total_results.append(result(0,0,0,0,0,0,0,0,0,0,0,0))\n",
    "#                 total_results.append(result(0,0,0,0,0,0,0,0,0,0,0,0))\n",
    "#                 single_results.append([])\n",
    "#                 results.append([])\n",
    "#             for t in range(T):\n",
    "#                 not_found = True\n",
    "#                 while not_found:\n",
    "#                     sample_test_X, sample_test_y = subsample(test_X, test_y, test_size)\n",
    "#                     if torch.sum(sample_test_y)>0 and torch.sum(sample_test_y)<test_size:\n",
    "#                         not_found = False\n",
    "                \n",
    "#                 get_err_score(train_data, sample_test_X, sample_test_y, num_candidate)\n",
    "#                 sorted_score = sorted(train_data, key=lambda x: x.score, reverse = True)\n",
    "#                 sorted_post_loss = sorted(train_data, key=lambda x: x.post_loss, reverse = False)\n",
    "#                 sorted_base_loss = sorted(train_data, key=lambda x: x.base_loss, reverse = False)\n",
    "#                 sorted_smooth = sorted(train_data, key=lambda x: x.smooth, reverse = False)\n",
    "#                 sorted_post_acc = sorted(train_data, key=lambda x: x.post_acc, reverse = True)\n",
    "#                 sorted_base_acc = sorted(train_data, key=lambda x: x.base_acc, reverse = True)\n",
    "\n",
    "#                 for i in range(len(top_candidate)):\n",
    "#                     for j in range(top_candidate[i]):\n",
    "#                         total_noise_ratio[i] += np.array([sorted_score[j].noise_ratio, sorted_post_loss[j].noise_ratio, sorted_base_loss[j].noise_ratio, sorted_smooth[j].noise_ratio, sorted_post_acc[j].noise_ratio, sorted_base_acc[j].noise_ratio])\n",
    "#                         total_label_mae[i] += np.array([sorted_score[j].label_mae, sorted_post_loss[j].label_mae, sorted_base_loss[j].label_mae, sorted_smooth[j].label_mae, sorted_post_acc[j].label_mae, sorted_base_acc[j].label_mae])\n",
    "#                         total_bias_mae[i] += np.array([sorted_score[j].bias_mae, sorted_post_loss[j].bias_mae, sorted_base_loss[j].bias_mae, sorted_smooth[j].bias_mae, sorted_post_acc[j].bias_mae, sorted_base_acc[j].bias_mae])\n",
    "\n",
    "#                 score_X = sorted_score[0].X\n",
    "#                 score_y = sorted_score[0].y\n",
    "#                 post_loss_X = sorted_post_loss[0].X\n",
    "#                 post_loss_y = sorted_post_loss[0].y\n",
    "#                 base_loss_X = sorted_base_loss[0].X\n",
    "#                 base_loss_y = sorted_base_loss[0].y\n",
    "#                 smooth_X = sorted_smooth[0].X\n",
    "#                 smooth_y = sorted_smooth[0].y\n",
    "#                 post_acc_X = sorted_post_acc[0].X\n",
    "#                 post_acc_y = sorted_post_acc[0].y\n",
    "#                 base_acc_X = sorted_base_acc[0].X\n",
    "#                 base_acc_y = sorted_base_acc[0].y\n",
    "\n",
    "#                 for j in range(1, top_candidate[-1]):\n",
    "#                     score_X = torch.concatenate((score_X, sorted_score[j].X), axis = 0)\n",
    "#                     score_y = torch.concatenate((score_y, sorted_score[j].y), axis = 0)\n",
    "#                     post_loss_X = torch.concatenate((post_loss_X, sorted_post_loss[j].X), axis = 0)\n",
    "#                     post_loss_y = torch.concatenate((post_loss_y, sorted_post_loss[j].y), axis = 0)\n",
    "#                     base_loss_X = torch.concatenate((base_loss_X, sorted_base_loss[j].X), axis = 0)\n",
    "#                     base_loss_y = torch.concatenate((base_loss_y, sorted_base_loss[j].y), axis = 0)\n",
    "#                     smooth_X = torch.concatenate((smooth_X, sorted_smooth[j].X), axis = 0)\n",
    "#                     smooth_y = torch.concatenate((smooth_y, sorted_smooth[j].y), axis = 0)\n",
    "#                     post_acc_X = torch.concatenate((post_acc_X, sorted_post_acc[j].X), axis = 0)\n",
    "#                     post_acc_y = torch.concatenate((post_acc_y, sorted_post_acc[j].y), axis = 0)\n",
    "#                     base_acc_X = torch.concatenate((base_acc_X, sorted_base_acc[j].X), axis = 0)\n",
    "#                     base_acc_y = torch.concatenate((base_acc_y, sorted_base_acc[j].y), axis = 0)\n",
    "\n",
    "#                 for i in range(len(top_candidate)):\n",
    "#                     single_score_loss, single_score_acc, score_loss, score_acc = compute_data_err_acc(test_X, test_y, score_X[:int(top_candidate[i]*train_size)], score_y[:int(top_candidate[i]*train_size)])\n",
    "#                     single_post_loss_loss, single_post_loss_acc, post_loss_loss, post_loss_acc = compute_data_err_acc(test_X, test_y, post_loss_X[:int(top_candidate[i]*train_size)], post_loss_y[:int(top_candidate[i]*train_size)])\n",
    "#                     single_base_loss_loss, single_base_loss_acc, base_loss_loss, base_loss_acc = compute_data_err_acc(test_X, test_y, base_loss_X[:int(top_candidate[i]*train_size)], base_loss_y[:int(top_candidate[i]*train_size)])\n",
    "#                     single_smooth_loss, single_smooth_acc, smooth_loss, smooth_acc = compute_data_err_acc(test_X, test_y, smooth_X[:int(top_candidate[i]*train_size)], smooth_y[:int(top_candidate[i]*train_size)])\n",
    "#                     single_post_acc_loss, single_post_acc_acc, post_acc_loss, post_acc_acc = compute_data_err_acc(test_X, test_y, post_acc_X[:int(top_candidate[i]*train_size)], post_acc_y[:int(top_candidate[i]*train_size)])\n",
    "#                     single_base_acc_loss, single_base_acc_acc, base_acc_loss, base_acc_acc = compute_data_err_acc(test_X, test_y, base_acc_X[:int(top_candidate[i]*train_size)], base_acc_y[:int(top_candidate[i]*train_size)])\n",
    "\n",
    "#                     single_resi = result(single_score_loss, single_score_acc, single_post_loss_loss, single_post_loss_acc, single_base_loss_loss, single_base_loss_acc, single_smooth_loss, single_smooth_acc, single_post_acc_loss, single_post_acc_acc, single_base_acc_loss, single_base_acc_acc)\n",
    "#                     resi = result(score_loss, score_acc, post_loss_loss, post_loss_acc, base_loss_loss, base_loss_acc, smooth_loss, smooth_acc, post_acc_loss, post_acc_acc, base_acc_loss, base_acc_acc)\n",
    "#                     single_results[i].append(single_resi)\n",
    "#                     results[i].append(resi)\n",
    "#                     single_total_results[i].add(single_resi)\n",
    "#                     total_results[i].add(resi)\n",
    "\n",
    "#                     # print(single_resi.getacc(), single_resi.getloss(), resi.getacc(), resi.getloss())\n",
    "#                     # sys.exit()\n",
    "#             for i in range(len(top_candidate)):\n",
    "#                 single_total_results[i].divide(T)\n",
    "#                 total_results[i].divide(T)\n",
    "#                 single_summ = summary([0, 0], [0, 0], [0, 0], [0, 0], single_total_results[i])\n",
    "#                 summ = summary([0, 0], [0, 0], [0, 0], [0, 0], total_results[i])\n",
    "\n",
    "#                 for t in range(T):\n",
    "#                     if single_results[i][t].smooth_acc >= single_results[i][t].post_loss_acc:\n",
    "#                         single_summ.percentage_smooth_post[1] += 1/T*100\n",
    "#                     if single_results[i][t].smooth_acc > single_results[i][t].post_loss_acc:\n",
    "#                         single_summ.percentage_smooth_post[0] += 1/T*100\n",
    "#                     if single_results[i][t].score_acc >= single_results[i][t].post_loss_acc:\n",
    "#                         single_summ.percentage_score_post[1] += 1/T*100\n",
    "#                     if single_results[i][t].score_acc > single_results[i][t].post_loss_acc:\n",
    "#                         single_summ.percentage_score_post[0] += 1/T*100\n",
    "#                     if single_results[i][t].post_loss_acc >= single_results[i][t].base_loss_acc:\n",
    "#                         single_summ.percentage_post_base_loss[1] += 1/T*100\n",
    "#                     if single_results[i][t].post_loss_acc > single_results[i][t].base_loss_acc:\n",
    "#                         single_summ.percentage_post_base_loss[0] += 1/T*100\n",
    "#                     if single_results[i][t].post_acc_acc >= single_results[i][t].base_acc_acc:\n",
    "#                         single_summ.percentage_post_base_acc[1] += 1/T*100\n",
    "#                     if single_results[i][t].post_acc_acc > single_results[i][t].base_acc_acc:\n",
    "#                         single_summ.percentage_post_base_acc[0] += 1/T*100\n",
    "\n",
    "#                     if results[i][t].smooth_acc >= results[i][t].post_loss_acc:\n",
    "#                         summ.percentage_smooth_post[1] += 1/T*100\n",
    "#                     if results[i][t].smooth_acc > results[i][t].post_loss_acc:\n",
    "#                         summ.percentage_smooth_post[0] += 1/T*100\n",
    "#                     if results[i][t].score_acc >= results[i][t].post_loss_acc:\n",
    "#                         summ.percentage_score_post[1] += 1/T*100\n",
    "#                     if results[i][t].score_acc > results[i][t].post_loss_acc:\n",
    "#                         summ.percentage_score_post[0] += 1/T*100\n",
    "#                     if results[i][t].post_loss_acc >= results[i][t].base_loss_acc:\n",
    "#                         summ.percentage_post_base_loss[1] += 1/T*100\n",
    "#                     if results[i][t].post_loss_acc > results[i][t].base_loss_acc:\n",
    "#                         summ.percentage_post_base_loss[0] += 1/T*100\n",
    "#                     if results[i][t].post_acc_acc >= results[i][t].base_acc_acc:\n",
    "#                         summ.percentage_post_base_acc[1] += 1/T*100\n",
    "#                     if results[i][t].post_acc_acc > results[i][t].base_acc_acc:\n",
    "#                         summ.percentage_post_base_acc[0] += 1/T*100\n",
    "#                 single_total_summary[i].add(single_summ)\n",
    "#                 total_summary[i].add(summ)\n",
    "#         for i in range(len(top_candidate)):\n",
    "#             single_total_summary[i].divide(D)\n",
    "#             total_summary[i].divide(D)\n",
    "#             total_noise_ratio[i] = total_noise_ratio[i] / (T*D*top_candidate[i])\n",
    "#             total_label_mae[i] = total_label_mae[i] / (T*D*top_candidate[i])\n",
    "#             total_bias_mae[i] = total_bias_mae[i] / (T*D*top_candidate[i])\n",
    "#             label = str(top_candidate[i]) + \", \" + str(test_size) + \", \" + str(num_candidate)\n",
    "#             x_label.append(label)\n",
    "#             single_acc = single_total_summary[i].getacc()\n",
    "#             acc = total_summary[i].getacc()\n",
    "#             single_loss = single_total_summary[i].getloss()\n",
    "#             loss = total_summary[i].getloss()\n",
    "#             print(\"Acc \" + \"& $\"+label+\"$ & \" + \"{:.4f}\".format(single_acc[0]) + \" & \" + \"{:.4f}\".format(single_acc[1]) + \" & \" + \"{:.4f}\".format(single_acc[2]) + \" & \" + \"{:.4f}\".format(single_acc[3]) + \" & \" + \"{:.4f}\".format(single_acc[4]) + \" & \" + \"{:.4f}\".format(single_acc[5]) +\" \\\\\\\\ \" )\n",
    "#             print(\"Aver. Acc \" + \"& $\"+label+\"$ & \" + \"{:.4f}\".format(acc[0]) + \" & \" + \"{:.4f}\".format(acc[1]) + \" & \" + \"{:.4f}\".format(acc[2]) + \" & \" + \"{:.4f}\".format(acc[3]) + \" & \" + \"{:.4f}\".format(acc[4]) + \" & \" + \"{:.4f}\".format(acc[5]) +\" \\\\\\\\ \" )\n",
    "#             # print(\"$\"+label+\"$ & \" + \"{:.4f}\".format(single_acc[0]-acc[0]) + \" & \" + \"{:.4f}\".format(single_acc[1]-acc[1]) + \" & \" + \"{:.4f}\".format(single_acc[2]-acc[2]) + \" & \" + \"{:.4f}\".format(single_acc[3]-acc[3]) + \" & \" + \"{:.4f}\".format(single_acc[4]-acc[4]) + \" & \" + \"{:.4f}\".format(single_acc[5]-acc[5]) + \" \\\\\\\\ \" )\n",
    "#             print(\"Loss \" + \"& $\"+label+\"$ & \" + \"{:.4f}\".format(single_loss[0]) + \" & \" + \"{:.4f}\".format(single_loss[1]) + \" & \" + \"{:.4f}\".format(single_loss[2]) + \" & \" + \"{:.4f}\".format(single_loss[3]) + \" & \" + \"{:.4f}\".format(single_loss[4]) + \" & \" + \"{:.4f}\".format(single_loss[5]) +\" \\\\\\\\ \" )\n",
    "#             print(\"Aver. Loss \" + \"& $\"+label+\"$ & \" + \"{:.4f}\".format(loss[0]) + \" & \" + \"{:.4f}\".format(loss[1]) + \" & \" + \"{:.4f}\".format(loss[2]) + \" & \" + \"{:.4f}\".format(loss[3]) + \" & \" + \"{:.4f}\".format(loss[4]) + \" & \" + \"{:.4f}\".format(loss[5]) +\" \\\\\\\\ \" )\n",
    "#             print(\"Noise Ratio \" + \"& $\" + label + \"$ & \" + \"{:.4f}\".format(total_noise_ratio[i][0]) + \" & \" + \"{:.4f}\".format(total_noise_ratio[i][1]) + \" & \" + \"{:.4f}\".format(total_noise_ratio[i][2]) + \" & \" + \"{:.4f}\".format(total_noise_ratio[i][3]) + \" & \" + \"{:.4f}\".format(total_noise_ratio[i][4]) + \" & \" + \"{:.4f}\".format(total_noise_ratio[i][5]) +\" \\\\\\\\ \" )\n",
    "#             print(\"Label MAE \" + \"& $\" + label + \"$ & \" + \"{:.4f}\".format(total_label_mae[i][0]) + \" & \" + \"{:.4f}\".format(total_label_mae[i][1]) + \" & \" + \"{:.4f}\".format(total_label_mae[i][2]) + \" & \" + \"{:.4f}\".format(total_label_mae[i][3]) + \" & \" + \"{:.4f}\".format(total_label_mae[i][4]) + \" & \" + \"{:.4f}\".format(total_label_mae[i][5]) +\" \\\\\\\\ \" + \" \\\\hline\")\n",
    "#             print(\"Bias MAE \" + \"& $\" + label + \"$ & \" + \"{:.4f}\".format(total_bias_mae[i][0]) + \" & \" + \"{:.4f}\".format(total_bias_mae[i][1]) + \" & \" + \"{:.4f}\".format(total_bias_mae[i][2]) + \" & \" + \"{:.4f}\".format(total_bias_mae[i][3]) + \" & \" + \"{:.4f}\".format(total_bias_mae[i][4]) + \" & \" + \"{:.4f}\".format(total_bias_mae[i][5]) +\" \\\\\\\\ \" + \" \\\\hline\")\n",
    "#             # print(\"$\"+label+\"$ & \" + \"{:.4f}\".format(single_loss[0]-loss[0]) + \" & \" + \"{:.4f}\".format(single_loss[1]-loss[1]) + \" & \" + \"{:.4f}\".format(single_loss[2]-loss[2]) + \" & \" + \"{:.4f}\".format(single_loss[3]-loss[3]) + \" & \" + \"{:.4f}\".format(single_loss[4]-loss[4]) + \" & \" + \"{:.4f}\".format(single_loss[5]-loss[5]) + \" \\\\\\\\ \" )\n",
    "#             single_smooth_post_L.append(single_total_summary[i].percentage_smooth_post[0])\n",
    "#             single_smooth_post_H.append(single_total_summary[i].percentage_smooth_post[1])\n",
    "#             single_score_post_L.append(single_total_summary[i].percentage_score_post[0])\n",
    "#             single_score_post_H.append(single_total_summary[i].percentage_score_post[1])\n",
    "#             single_post_base_loss_L.append(single_total_summary[i].percentage_post_base_loss[0])\n",
    "#             single_post_base_loss_H.append(single_total_summary[i].percentage_post_base_loss[1])\n",
    "#             single_post_base_acc_L.append(single_total_summary[i].percentage_post_base_acc[0])\n",
    "#             single_post_base_acc_H.append(single_total_summary[i].percentage_post_base_acc[1])\n",
    "\n",
    "#             smooth_post_L.append(total_summary[i].percentage_smooth_post[0])\n",
    "#             smooth_post_H.append(total_summary[i].percentage_smooth_post[1])\n",
    "#             score_post_L.append(total_summary[i].percentage_score_post[0])\n",
    "#             score_post_H.append(total_summary[i].percentage_score_post[1])\n",
    "#             post_base_loss_L.append(total_summary[i].percentage_post_base_loss[0])\n",
    "#             post_base_loss_H.append(total_summary[i].percentage_post_base_loss[1])\n",
    "#             post_base_acc_L.append(total_summary[i].percentage_post_base_acc[0])\n",
    "#             post_base_acc_H.append(total_summary[i].percentage_post_base_acc[1])\n",
    "\n",
    "#             single_acc_all.append(single_acc)\n",
    "#             acc_all.append(acc)\n",
    "#             single_loss_all.append(single_loss)\n",
    "#             loss_all.append(loss)\n",
    "#             noise_ratio_all.append(total_noise_ratio[i])\n",
    "#             label_mae_all.append(total_label_mae[i])\n",
    "#             bias_mae_all.append(total_bias_mae[i])\n",
    "#         # profiler.stop()\n",
    "\n",
    "#         # profiler.print()\n",
    "#         # sys.exit()\n",
    "    \n",
    "#     x_label_all += x_label\n",
    "#     single_smooth_post_L_all += single_smooth_post_L\n",
    "#     single_smooth_post_H_all += single_smooth_post_H\n",
    "#     single_score_post_L_all += single_score_post_L\n",
    "#     single_score_post_H_all += single_score_post_H\n",
    "#     single_post_base_loss_L_all += single_post_base_loss_L\n",
    "#     single_post_base_loss_H_all += single_post_base_loss_H\n",
    "#     single_post_base_acc_L_all += single_post_base_acc_L\n",
    "#     single_post_base_acc_H_all += single_post_base_acc_H\n",
    "\n",
    "#     smooth_post_L_all += smooth_post_L\n",
    "#     smooth_post_H_all += smooth_post_H\n",
    "#     score_post_L_all += score_post_L\n",
    "#     score_post_H_all += score_post_H\n",
    "#     post_base_loss_L_all += post_base_loss_L\n",
    "#     post_base_loss_H_all += post_base_loss_H\n",
    "#     post_base_acc_L_all += post_base_acc_L\n",
    "#     post_base_acc_H_all += post_base_acc_H\n",
    "\n",
    "#     single_acc_all_all += single_acc_all\n",
    "#     acc_all_all += acc_all\n",
    "#     single_loss_all_all += single_loss_all\n",
    "#     loss_all_all += loss_all\n",
    "#     noise_ratio_all_all += noise_ratio_all\n",
    "#     label_mae_all_all += label_mae_all\n",
    "#     bias_mae_all_all += bias_mae_all\n",
    "\n",
    "#     generate_plot(x_label, single_smooth_post_L, single_smooth_post_H, 'Posterior Predictive', 'Small Smooth', 'single_smooth_post')\n",
    "#     generate_plot(x_label, single_score_post_L, single_score_post_H, 'Posterior Predictive', 'PMI', 'single_score_post')\n",
    "#     generate_plot(x_label, single_post_base_loss_L, single_post_base_loss_H, 'Cross Entropy', 'Average Cross Entropy', 'single_post_base_loss')\n",
    "#     generate_plot(x_label, single_post_base_acc_L, single_post_base_acc_H, 'Accuracy', 'Average Accuracy', 'single_post_base_acc')\n",
    "#     generate_plot(x_label, smooth_post_L, smooth_post_H, 'Posterior Predictive', 'Small Smooth', 'smooth_post')\n",
    "#     generate_plot(x_label, score_post_L, score_post_H, 'Posterior Predictive', 'PMI', 'score_post')\n",
    "#     generate_plot(x_label, post_base_loss_L, post_base_loss_H, 'Cross Entropy', 'Average Cross Entropy', 'post_base_loss')\n",
    "#     generate_plot(x_label, post_base_acc_L, post_base_acc_H, 'Accuracy', 'Average Accuracy', 'post_base_acc')\n",
    "\n",
    "# # profiler.stop()\n",
    "\n",
    "# # profiler.print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
